{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b20374ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d837446",
   "metadata": {},
   "source": [
    "This code loads your CSV file, splits the data into a training set and a test set, and creates a DataLoader for each. The DataLoader can be used to iterate through the data in batches, which is useful for training a neural network.\n",
    "\n",
    "You can replace 'yourfile.csv' with the path to your actual file. Also, note that this assumes your CSV file doesn't have a header. If it does, you might need to skip the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7bd7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccelDataset(Dataset):\n",
    "    def __init__(self, data, labels, sequence_length=10):\n",
    "        self.data = [data[i:i+sequence_length] for i in range(len(data) - sequence_length + 1)]\n",
    "        self.labels = labels[(int)(sequence_length/2) - 1 : len(data) - (sequence_length - (int)(sequence_length/2))]\n",
    "        # change to get the majority\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].transpose(0, 1), self.labels[idx]  # Transposing the sequence and channel dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc4ed681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22292\n",
      "\n",
      "{0: 21276, 1: 1016}\n",
      "1016\n",
      "{0: 21244, 1: 1048}\n",
      "1048\n",
      "{0: 20288, 1: 2004}\n",
      "2004\n",
      "{0: 20350, 1: 1942}\n",
      "1942\n",
      "{0: 22006, 1: 286}\n",
      "286\n",
      "{0: 22022, 1: 270}\n",
      "270\n",
      "{0: 8990, 1: 13302}\n",
      "13302\n",
      "{0: 19869, 1: 2423}\n",
      "2423\n",
      "\n",
      "1016\n",
      "22292\n",
      "1048\n",
      "22292\n",
      "1495\n",
      "21783\n",
      "1495\n",
      "21336\n",
      "286\n",
      "21336\n",
      "270\n",
      "21336\n",
      "1495\n",
      "9529\n",
      "1495\n",
      "8601\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "WORKAREA_PATH = './'\n",
    "class_num = 8 ## Although annoying, requiring this be manually adjusted\n",
    "    ## to open the file also means that a later instance of this number will be correct\n",
    "dataframe = pd.read_csv(WORKAREA_PATH + f'Data/COMBINED_Type5-WithClassNum{class_num}-Freq10-Labeled_Motion-sessions_23-24_Fall.csv')\n",
    "# code test file: Data/Week 1/Left then Right/Processed/Type3-Freq10-Labeled_Motion-sessions_2023-08-26_17-25-54.csv\n",
    "# classifier training file: Data/COMBINED_Type3-Freq10-Labeled_Motion-sessions_23-24_Fall.csv\n",
    "# LIST OF FULL-SIZED FILES:\n",
    "    # Data/COMBINED_Type3-Freq10-Labeled_Motion-sessions_23-24_Fall.csv\n",
    "    # Data/COMBINED_Type5-WithClassNum{class_num}-Freq10-Labeled_Motion-sessions_23-24_Fall.csv\n",
    "\n",
    "print(len(dataframe))\n",
    "print()\n",
    "# print(dataframe.columns)\n",
    "dataframe.columns = pd.Index(np.arange(len(dataframe.columns)), dtype='int64')\n",
    "# print(dataframe.columns)\n",
    "def skcounter(df):\n",
    "    skarr = np.unique(df, return_counts = True)\n",
    "    # print(skarr)\n",
    "    # find_index = np.where(skarr[0], True, False)\n",
    "    # for i in range(len(find_index)):\n",
    "    #     if find_index[i]: ret_valsk = skarr[1][i]\n",
    "    ret_dict = dict(zip(skarr[0], skarr[1]))\n",
    "    print(ret_dict)\n",
    "    ret_valsk = ret_dict.get(1, 0)\n",
    "    print(ret_valsk)\n",
    "    # print(dataframe)\n",
    "    return ret_valsk\n",
    "avg_count = int(np.median(dataframe[dataframe.columns[3 : ]].apply(lambda x: skcounter(x)).to_numpy()))\n",
    "# class_counts = {}\n",
    "# for i in dataframe.columns[input_num : ]:\n",
    "#     if i == SKDescriptors.STATIONARY_CLASS:\n",
    "#         stationary_rows = dataframe[dataframe[i] == 1]\n",
    "#         continue\n",
    "#     class_counts[i] = len(dataframe[dataframe[i] == 1])\n",
    "\n",
    "print()\n",
    "for i in dataframe.columns[3 : ]:\n",
    "    class_i_rows = dataframe[dataframe[i] == 1]\n",
    "    # this is the line doing the actual randomization\n",
    "    sample_rows = class_i_rows.sample(min(len(class_i_rows), avg_count), random_state=42)\n",
    "    print(len(sample_rows))\n",
    "    dataframe = dataframe.drop(class_i_rows.drop(sample_rows.index).index)\n",
    "    print(len(dataframe))\n",
    "\n",
    "\n",
    "# stat and other adjustments only\n",
    "# stationary_rows = dataframe[dataframe[len(dataframe.columns) - 2] == 1]\n",
    "# other_rows = dataframe[dataframe[len(dataframe.columns) - 1] == 1]\n",
    "# print()\n",
    "# print(avg_count)\n",
    "# print(\"statlen: \" + str(len(stationary_rows)))\n",
    "# print(\"otherlen: \" + str(len(other_rows)))\n",
    "# # this is the line doing the actual randomization\n",
    "# sample_rows = stationary_rows.sample(avg_count, random_state=42)\n",
    "# print(len(sample_rows))\n",
    "# dataframe = dataframe.drop(stationary_rows.drop(sample_rows.index).index)\n",
    "# print(len(dataframe))\n",
    "# # this is the line doing the actual randomization\n",
    "# sample_rows = other_rows.sample(avg_count, random_state=42)\n",
    "# print(len(sample_rows))\n",
    "# dataframe = dataframe.drop(other_rows.drop(sample_rows.index).index)\n",
    "# print(len(dataframe))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get data and labels from dataframe\n",
    "data = dataframe.iloc[:, :3].values  # x, y, z data\n",
    "labels = dataframe.iloc[:, 3:].values  # labels\n",
    "\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "# Split data into training and test sets\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to tensors\n",
    "data_train = torch.tensor(data_train, dtype=torch.float32)  \n",
    "data_test = torch.tensor(data_test, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to tensors and get max index (assuming one-hot encoding)\n",
    "labels_train = torch.argmax(torch.tensor(labels_train, dtype=torch.float32), dim=1)\n",
    "labels_test = torch.argmax(torch.tensor(labels_test, dtype=torch.float32), dim=1)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = AccelDataset(data_train, labels_train, sequence_length)\n",
    "test_dataset = AccelDataset(data_test, labels_test, sequence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b420a11",
   "metadata": {},
   "source": [
    "In this code, we added 2 more convolutional layers, which can extract more complex features from your accelerometer data. The number of output channels in the convolutional layers gradually increases, as it is common in many deep learning models to gradually increase the complexity and decrease the spatial size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7b69385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(3, 64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        \n",
    "        # Adjust the fully connected layer's input size based on the new sequence length after convolutions.\n",
    "        # Adjusted for sequence length = 4 after 3 conv layers with kernel size 3\n",
    "        # 10 -3 + 1 = 8 after the first layer\n",
    "        # 8 - 3 + 1 = 6 after the second layer\n",
    "        # 6 - 3 + 1 = 4 after the third layer\n",
    "        self.fc1 = nn.Linear(256 * (sequence_length - 6), 128)  # Adjusted for sequence length = 4 after 3 conv layers with kernel size 3\n",
    "        self.fc2 = nn.Linear(128, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)  # Apply softmax to the output layer\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1ec858",
   "metadata": {},
   "source": [
    "Please note that the dimension of the input to the fully connected layer depends on the output size of your last convolutional layer. This code assumes that after 3 layers of convolution with kernel size 5 and stride 1, the sequence length is reduced to 82 (from the original 100). You may need to adjust this according to your own situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24ab9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     2] loss: 0.001\n",
      "[1,     3] loss: 0.001\n",
      "[1,     4] loss: 0.001\n",
      "[1,     5] loss: 0.001\n",
      "[1,     6] loss: 0.001\n",
      "[1,     7] loss: 0.001\n",
      "[1,     8] loss: 0.001\n",
      "[1,     9] loss: 0.001\n",
      "[1,    10] loss: 0.001\n",
      "[1,    11] loss: 0.001\n",
      "[1,    12] loss: 0.001\n",
      "[1,    13] loss: 0.001\n",
      "[1,    14] loss: 0.001\n",
      "[1,    15] loss: 0.001\n",
      "[1,    16] loss: 0.001\n",
      "[1,    17] loss: 0.001\n",
      "[1,    18] loss: 0.001\n",
      "[1,    19] loss: 0.001\n",
      "[1,    20] loss: 0.001\n",
      "[1,    21] loss: 0.001\n",
      "[1,    22] loss: 0.001\n",
      "[1,    23] loss: 0.001\n",
      "[1,    24] loss: 0.001\n",
      "[1,    25] loss: 0.001\n",
      "[1,    26] loss: 0.001\n",
      "[1,    27] loss: 0.001\n",
      "[1,    28] loss: 0.001\n",
      "[1,    29] loss: 0.001\n",
      "[1,    30] loss: 0.001\n",
      "[1,    31] loss: 0.001\n",
      "[1,    32] loss: 0.001\n",
      "[1,    33] loss: 0.001\n",
      "[1,    34] loss: 0.001\n",
      "[1,    35] loss: 0.001\n",
      "[1,    36] loss: 0.001\n",
      "[1,    37] loss: 0.001\n",
      "[1,    38] loss: 0.001\n",
      "[1,    39] loss: 0.001\n",
      "[1,    40] loss: 0.001\n",
      "[1,    41] loss: 0.001\n",
      "[1,    42] loss: 0.001\n",
      "[1,    43] loss: 0.001\n",
      "[1,    44] loss: 0.001\n",
      "[1,    45] loss: 0.001\n",
      "[1,    46] loss: 0.001\n",
      "[1,    47] loss: 0.001\n",
      "[1,    48] loss: 0.001\n",
      "[1,    49] loss: 0.001\n",
      "[1,    50] loss: 0.001\n",
      "[1,    51] loss: 0.001\n",
      "[1,    52] loss: 0.001\n",
      "[1,    53] loss: 0.001\n",
      "[1,    54] loss: 0.001\n",
      "[1,    55] loss: 0.001\n",
      "[1,    56] loss: 0.001\n",
      "[1,    57] loss: 0.001\n",
      "[1,    58] loss: 0.001\n",
      "[1,    59] loss: 0.001\n",
      "[1,    60] loss: 0.001\n",
      "[1,    61] loss: 0.001\n",
      "[1,    62] loss: 0.001\n",
      "[1,    63] loss: 0.001\n",
      "[1,    64] loss: 0.001\n",
      "[1,    65] loss: 0.001\n",
      "[1,    66] loss: 0.001\n",
      "[1,    67] loss: 0.001\n",
      "[1,    68] loss: 0.001\n",
      "[1,    69] loss: 0.001\n",
      "[1,    70] loss: 0.001\n",
      "[1,    71] loss: 0.001\n",
      "[1,    72] loss: 0.001\n",
      "[1,    73] loss: 0.001\n",
      "[1,    74] loss: 0.001\n",
      "[1,    75] loss: 0.001\n",
      "[1,    76] loss: 0.001\n",
      "[1,    77] loss: 0.001\n",
      "[1,    78] loss: 0.001\n",
      "[1,    79] loss: 0.001\n",
      "[1,    80] loss: 0.001\n",
      "[1,    81] loss: 0.001\n",
      "[1,    82] loss: 0.001\n",
      "[1,    83] loss: 0.001\n",
      "[1,    84] loss: 0.001\n",
      "[1,    85] loss: 0.001\n",
      "[1,    86] loss: 0.001\n",
      "[1,    87] loss: 0.001\n",
      "[1,    88] loss: 0.001\n",
      "[1,    89] loss: 0.001\n",
      "[1,    90] loss: 0.001\n",
      "[1,    91] loss: 0.001\n",
      "[1,    92] loss: 0.001\n",
      "[1,    93] loss: 0.001\n",
      "[1,    94] loss: 0.001\n",
      "[1,    95] loss: 0.001\n",
      "[1,    96] loss: 0.001\n",
      "[1,    97] loss: 0.001\n",
      "[1,    98] loss: 0.001\n",
      "[1,    99] loss: 0.001\n",
      "[1,   100] loss: 0.001\n",
      "[1,   101] loss: 0.001\n",
      "[1,   102] loss: 0.001\n",
      "[1,   103] loss: 0.001\n",
      "[1,   104] loss: 0.001\n",
      "[1,   105] loss: 0.001\n",
      "[1,   106] loss: 0.001\n",
      "[1,   107] loss: 0.001\n",
      "[1,   108] loss: 0.001\n",
      "[201,     1] loss: 0.001\n",
      "[201,     2] loss: 0.001\n",
      "[201,     3] loss: 0.001\n",
      "[201,     4] loss: 0.001\n",
      "[201,     5] loss: 0.001\n",
      "[201,     6] loss: 0.001\n",
      "[201,     7] loss: 0.001\n",
      "[201,     8] loss: 0.001\n",
      "[201,     9] loss: 0.001\n",
      "[201,    10] loss: 0.001\n",
      "[201,    11] loss: 0.001\n",
      "[201,    12] loss: 0.001\n",
      "[201,    13] loss: 0.001\n",
      "[201,    14] loss: 0.001\n",
      "[201,    15] loss: 0.001\n",
      "[201,    16] loss: 0.001\n",
      "[201,    17] loss: 0.001\n",
      "[201,    18] loss: 0.001\n",
      "[201,    19] loss: 0.001\n",
      "[201,    20] loss: 0.001\n",
      "[201,    21] loss: 0.001\n",
      "[201,    22] loss: 0.001\n",
      "[201,    23] loss: 0.001\n",
      "[201,    24] loss: 0.001\n",
      "[201,    25] loss: 0.001\n",
      "[201,    26] loss: 0.001\n",
      "[201,    27] loss: 0.001\n",
      "[201,    28] loss: 0.001\n",
      "[201,    29] loss: 0.001\n",
      "[201,    30] loss: 0.001\n",
      "[201,    31] loss: 0.001\n",
      "[201,    32] loss: 0.001\n",
      "[201,    33] loss: 0.001\n",
      "[201,    34] loss: 0.001\n",
      "[201,    35] loss: 0.001\n",
      "[201,    36] loss: 0.001\n",
      "[201,    37] loss: 0.001\n",
      "[201,    38] loss: 0.001\n",
      "[201,    39] loss: 0.001\n",
      "[201,    40] loss: 0.001\n",
      "[201,    41] loss: 0.001\n",
      "[201,    42] loss: 0.001\n",
      "[201,    43] loss: 0.001\n",
      "[201,    44] loss: 0.001\n",
      "[201,    45] loss: 0.001\n",
      "[201,    46] loss: 0.001\n",
      "[201,    47] loss: 0.001\n",
      "[201,    48] loss: 0.001\n",
      "[201,    49] loss: 0.001\n",
      "[201,    50] loss: 0.001\n",
      "[201,    51] loss: 0.001\n",
      "[201,    52] loss: 0.001\n",
      "[201,    53] loss: 0.001\n",
      "[201,    54] loss: 0.001\n",
      "[201,    55] loss: 0.001\n",
      "[201,    56] loss: 0.001\n",
      "[201,    57] loss: 0.001\n",
      "[201,    58] loss: 0.001\n",
      "[201,    59] loss: 0.001\n",
      "[201,    60] loss: 0.001\n",
      "[201,    61] loss: 0.001\n",
      "[201,    62] loss: 0.001\n",
      "[201,    63] loss: 0.001\n",
      "[201,    64] loss: 0.001\n",
      "[201,    65] loss: 0.001\n",
      "[201,    66] loss: 0.001\n",
      "[201,    67] loss: 0.001\n",
      "[201,    68] loss: 0.001\n",
      "[201,    69] loss: 0.001\n",
      "[201,    70] loss: 0.001\n",
      "[201,    71] loss: 0.001\n",
      "[201,    72] loss: 0.001\n",
      "[201,    73] loss: 0.001\n",
      "[201,    74] loss: 0.001\n",
      "[201,    75] loss: 0.001\n",
      "[201,    76] loss: 0.001\n",
      "[201,    77] loss: 0.001\n",
      "[201,    78] loss: 0.001\n",
      "[201,    79] loss: 0.001\n",
      "[201,    80] loss: 0.001\n",
      "[201,    81] loss: 0.001\n",
      "[201,    82] loss: 0.001\n",
      "[201,    83] loss: 0.001\n",
      "[201,    84] loss: 0.001\n",
      "[201,    85] loss: 0.001\n",
      "[201,    86] loss: 0.001\n",
      "[201,    87] loss: 0.001\n",
      "[201,    88] loss: 0.001\n",
      "[201,    89] loss: 0.001\n",
      "[201,    90] loss: 0.001\n",
      "[201,    91] loss: 0.001\n",
      "[201,    92] loss: 0.001\n",
      "[201,    93] loss: 0.001\n",
      "[201,    94] loss: 0.001\n",
      "[201,    95] loss: 0.001\n",
      "[201,    96] loss: 0.001\n",
      "[201,    97] loss: 0.001\n",
      "[201,    98] loss: 0.001\n",
      "[201,    99] loss: 0.001\n",
      "[201,   100] loss: 0.001\n",
      "[201,   101] loss: 0.001\n",
      "[201,   102] loss: 0.001\n",
      "[201,   103] loss: 0.001\n",
      "[201,   104] loss: 0.001\n",
      "[201,   105] loss: 0.001\n",
      "[201,   106] loss: 0.001\n",
      "[201,   107] loss: 0.001\n",
      "[201,   108] loss: 0.001\n",
      "[401,     1] loss: 0.001\n",
      "[401,     2] loss: 0.000\n",
      "[401,     3] loss: 0.000\n",
      "[401,     4] loss: 0.000\n",
      "[401,     5] loss: 0.000\n",
      "[401,     6] loss: 0.000\n",
      "[401,     7] loss: 0.000\n",
      "[401,     8] loss: 0.000\n",
      "[401,     9] loss: 0.000\n",
      "[401,    10] loss: 0.000\n",
      "[401,    11] loss: 0.000\n",
      "[401,    12] loss: 0.000\n",
      "[401,    13] loss: 0.000\n",
      "[401,    14] loss: 0.000\n",
      "[401,    15] loss: 0.000\n",
      "[401,    16] loss: 0.000\n",
      "[401,    17] loss: 0.000\n",
      "[401,    18] loss: 0.001\n",
      "[401,    19] loss: 0.000\n",
      "[401,    20] loss: 0.000\n",
      "[401,    21] loss: 0.000\n",
      "[401,    22] loss: 0.000\n",
      "[401,    23] loss: 0.000\n",
      "[401,    24] loss: 0.000\n",
      "[401,    25] loss: 0.000\n",
      "[401,    26] loss: 0.000\n",
      "[401,    27] loss: 0.000\n",
      "[401,    28] loss: 0.000\n",
      "[401,    29] loss: 0.000\n",
      "[401,    30] loss: 0.000\n",
      "[401,    31] loss: 0.000\n",
      "[401,    32] loss: 0.000\n",
      "[401,    33] loss: 0.000\n",
      "[401,    34] loss: 0.000\n",
      "[401,    35] loss: 0.000\n",
      "[401,    36] loss: 0.000\n",
      "[401,    37] loss: 0.000\n",
      "[401,    38] loss: 0.000\n",
      "[401,    39] loss: 0.000\n",
      "[401,    40] loss: 0.000\n",
      "[401,    41] loss: 0.000\n",
      "[401,    42] loss: 0.000\n",
      "[401,    43] loss: 0.000\n",
      "[401,    44] loss: 0.000\n",
      "[401,    45] loss: 0.000\n",
      "[401,    46] loss: 0.000\n",
      "[401,    47] loss: 0.000\n",
      "[401,    48] loss: 0.000\n",
      "[401,    49] loss: 0.000\n",
      "[401,    50] loss: 0.000\n",
      "[401,    51] loss: 0.001\n",
      "[401,    52] loss: 0.000\n",
      "[401,    53] loss: 0.000\n",
      "[401,    54] loss: 0.000\n",
      "[401,    55] loss: 0.000\n",
      "[401,    56] loss: 0.000\n",
      "[401,    57] loss: 0.000\n",
      "[401,    58] loss: 0.000\n",
      "[401,    59] loss: 0.000\n",
      "[401,    60] loss: 0.000\n",
      "[401,    61] loss: 0.000\n",
      "[401,    62] loss: 0.000\n",
      "[401,    63] loss: 0.000\n",
      "[401,    64] loss: 0.000\n",
      "[401,    65] loss: 0.000\n",
      "[401,    66] loss: 0.000\n",
      "[401,    67] loss: 0.000\n",
      "[401,    68] loss: 0.000\n",
      "[401,    69] loss: 0.000\n",
      "[401,    70] loss: 0.000\n",
      "[401,    71] loss: 0.000\n",
      "[401,    72] loss: 0.000\n",
      "[401,    73] loss: 0.000\n",
      "[401,    74] loss: 0.001\n",
      "[401,    75] loss: 0.000\n",
      "[401,    76] loss: 0.000\n",
      "[401,    77] loss: 0.000\n",
      "[401,    78] loss: 0.000\n",
      "[401,    79] loss: 0.000\n",
      "[401,    80] loss: 0.000\n",
      "[401,    81] loss: 0.000\n",
      "[401,    82] loss: 0.000\n",
      "[401,    83] loss: 0.000\n",
      "[401,    84] loss: 0.000\n",
      "[401,    85] loss: 0.000\n",
      "[401,    86] loss: 0.000\n",
      "[401,    87] loss: 0.000\n",
      "[401,    88] loss: 0.000\n",
      "[401,    89] loss: 0.000\n",
      "[401,    90] loss: 0.001\n",
      "[401,    91] loss: 0.001\n",
      "[401,    92] loss: 0.000\n",
      "[401,    93] loss: 0.000\n",
      "[401,    94] loss: 0.000\n",
      "[401,    95] loss: 0.000\n",
      "[401,    96] loss: 0.000\n",
      "[401,    97] loss: 0.001\n",
      "[401,    98] loss: 0.000\n",
      "[401,    99] loss: 0.000\n",
      "[401,   100] loss: 0.000\n",
      "[401,   101] loss: 0.000\n",
      "[401,   102] loss: 0.000\n",
      "[401,   103] loss: 0.000\n",
      "[401,   104] loss: 0.000\n",
      "[401,   105] loss: 0.000\n",
      "[401,   106] loss: 0.000\n",
      "[401,   107] loss: 0.000\n",
      "[401,   108] loss: 0.000\n",
      "[601,     1] loss: 0.000\n",
      "[601,     2] loss: 0.000\n",
      "[601,     3] loss: 0.000\n",
      "[601,     4] loss: 0.000\n",
      "[601,     5] loss: 0.000\n",
      "[601,     6] loss: 0.000\n",
      "[601,     7] loss: 0.000\n",
      "[601,     8] loss: 0.000\n",
      "[601,     9] loss: 0.000\n",
      "[601,    10] loss: 0.000\n",
      "[601,    11] loss: 0.000\n",
      "[601,    12] loss: 0.000\n",
      "[601,    13] loss: 0.000\n",
      "[601,    14] loss: 0.000\n",
      "[601,    15] loss: 0.000\n",
      "[601,    16] loss: 0.000\n",
      "[601,    17] loss: 0.000\n",
      "[601,    18] loss: 0.000\n",
      "[601,    19] loss: 0.000\n",
      "[601,    20] loss: 0.000\n",
      "[601,    21] loss: 0.000\n",
      "[601,    22] loss: 0.000\n",
      "[601,    23] loss: 0.000\n",
      "[601,    24] loss: 0.000\n",
      "[601,    25] loss: 0.000\n",
      "[601,    26] loss: 0.000\n",
      "[601,    27] loss: 0.000\n",
      "[601,    28] loss: 0.000\n",
      "[601,    29] loss: 0.000\n",
      "[601,    30] loss: 0.000\n",
      "[601,    31] loss: 0.000\n",
      "[601,    32] loss: 0.000\n",
      "[601,    33] loss: 0.000\n",
      "[601,    34] loss: 0.000\n",
      "[601,    35] loss: 0.000\n",
      "[601,    36] loss: 0.000\n",
      "[601,    37] loss: 0.000\n",
      "[601,    38] loss: 0.000\n",
      "[601,    39] loss: 0.000\n",
      "[601,    40] loss: 0.000\n",
      "[601,    41] loss: 0.000\n",
      "[601,    42] loss: 0.000\n",
      "[601,    43] loss: 0.000\n",
      "[601,    44] loss: 0.000\n",
      "[601,    45] loss: 0.000\n",
      "[601,    46] loss: 0.000\n",
      "[601,    47] loss: 0.000\n",
      "[601,    48] loss: 0.000\n",
      "[601,    49] loss: 0.000\n",
      "[601,    50] loss: 0.000\n",
      "[601,    51] loss: 0.000\n",
      "[601,    52] loss: 0.000\n",
      "[601,    53] loss: 0.000\n",
      "[601,    54] loss: 0.000\n",
      "[601,    55] loss: 0.000\n",
      "[601,    56] loss: 0.000\n",
      "[601,    57] loss: 0.000\n",
      "[601,    58] loss: 0.000\n",
      "[601,    59] loss: 0.000\n",
      "[601,    60] loss: 0.000\n",
      "[601,    61] loss: 0.000\n",
      "[601,    62] loss: 0.000\n",
      "[601,    63] loss: 0.000\n",
      "[601,    64] loss: 0.000\n",
      "[601,    65] loss: 0.000\n",
      "[601,    66] loss: 0.000\n",
      "[601,    67] loss: 0.000\n",
      "[601,    68] loss: 0.000\n",
      "[601,    69] loss: 0.000\n",
      "[601,    70] loss: 0.000\n",
      "[601,    71] loss: 0.000\n",
      "[601,    72] loss: 0.000\n",
      "[601,    73] loss: 0.000\n",
      "[601,    74] loss: 0.000\n",
      "[601,    75] loss: 0.000\n",
      "[601,    76] loss: 0.000\n",
      "[601,    77] loss: 0.000\n",
      "[601,    78] loss: 0.000\n",
      "[601,    79] loss: 0.000\n",
      "[601,    80] loss: 0.000\n",
      "[601,    81] loss: 0.000\n",
      "[601,    82] loss: 0.000\n",
      "[601,    83] loss: 0.000\n",
      "[601,    84] loss: 0.000\n",
      "[601,    85] loss: 0.000\n",
      "[601,    86] loss: 0.000\n",
      "[601,    87] loss: 0.000\n",
      "[601,    88] loss: 0.000\n",
      "[601,    89] loss: 0.000\n",
      "[601,    90] loss: 0.000\n",
      "[601,    91] loss: 0.000\n",
      "[601,    92] loss: 0.000\n",
      "[601,    93] loss: 0.000\n",
      "[601,    94] loss: 0.000\n",
      "[601,    95] loss: 0.000\n",
      "[601,    96] loss: 0.000\n",
      "[601,    97] loss: 0.000\n",
      "[601,    98] loss: 0.000\n",
      "[601,    99] loss: 0.000\n",
      "[601,   100] loss: 0.000\n",
      "[601,   101] loss: 0.000\n",
      "[601,   102] loss: 0.000\n",
      "[601,   103] loss: 0.000\n",
      "[601,   104] loss: 0.000\n",
      "[601,   105] loss: 0.000\n",
      "[601,   106] loss: 0.000\n",
      "[601,   107] loss: 0.000\n",
      "[601,   108] loss: 0.000\n",
      "[801,     1] loss: 0.000\n",
      "[801,     2] loss: 0.000\n",
      "[801,     3] loss: 0.000\n",
      "[801,     4] loss: 0.000\n",
      "[801,     5] loss: 0.000\n",
      "[801,     6] loss: 0.000\n",
      "[801,     7] loss: 0.000\n",
      "[801,     8] loss: 0.000\n",
      "[801,     9] loss: 0.000\n",
      "[801,    10] loss: 0.000\n",
      "[801,    11] loss: 0.000\n",
      "[801,    12] loss: 0.000\n",
      "[801,    13] loss: 0.000\n",
      "[801,    14] loss: 0.000\n",
      "[801,    15] loss: 0.000\n",
      "[801,    16] loss: 0.000\n",
      "[801,    17] loss: 0.000\n",
      "[801,    18] loss: 0.000\n",
      "[801,    19] loss: 0.000\n",
      "[801,    20] loss: 0.000\n",
      "[801,    21] loss: 0.000\n",
      "[801,    22] loss: 0.000\n",
      "[801,    23] loss: 0.000\n",
      "[801,    24] loss: 0.000\n",
      "[801,    25] loss: 0.000\n",
      "[801,    26] loss: 0.000\n",
      "[801,    27] loss: 0.000\n",
      "[801,    28] loss: 0.000\n",
      "[801,    29] loss: 0.000\n",
      "[801,    30] loss: 0.000\n",
      "[801,    31] loss: 0.000\n",
      "[801,    32] loss: 0.000\n",
      "[801,    33] loss: 0.000\n",
      "[801,    34] loss: 0.000\n",
      "[801,    35] loss: 0.000\n",
      "[801,    36] loss: 0.000\n",
      "[801,    37] loss: 0.000\n",
      "[801,    38] loss: 0.000\n",
      "[801,    39] loss: 0.000\n",
      "[801,    40] loss: 0.000\n",
      "[801,    41] loss: 0.000\n",
      "[801,    42] loss: 0.000\n",
      "[801,    43] loss: 0.000\n",
      "[801,    44] loss: 0.000\n",
      "[801,    45] loss: 0.000\n",
      "[801,    46] loss: 0.000\n",
      "[801,    47] loss: 0.000\n",
      "[801,    48] loss: 0.000\n",
      "[801,    49] loss: 0.000\n",
      "[801,    50] loss: 0.000\n",
      "[801,    51] loss: 0.000\n",
      "[801,    52] loss: 0.000\n",
      "[801,    53] loss: 0.000\n",
      "[801,    54] loss: 0.000\n",
      "[801,    55] loss: 0.000\n",
      "[801,    56] loss: 0.000\n",
      "[801,    57] loss: 0.000\n",
      "[801,    58] loss: 0.000\n",
      "[801,    59] loss: 0.000\n",
      "[801,    60] loss: 0.000\n",
      "[801,    61] loss: 0.000\n",
      "[801,    62] loss: 0.000\n",
      "[801,    63] loss: 0.000\n",
      "[801,    64] loss: 0.000\n",
      "[801,    65] loss: 0.000\n",
      "[801,    66] loss: 0.000\n",
      "[801,    67] loss: 0.000\n",
      "[801,    68] loss: 0.000\n",
      "[801,    69] loss: 0.000\n",
      "[801,    70] loss: 0.000\n",
      "[801,    71] loss: 0.000\n",
      "[801,    72] loss: 0.000\n",
      "[801,    73] loss: 0.000\n",
      "[801,    74] loss: 0.000\n",
      "[801,    75] loss: 0.000\n",
      "[801,    76] loss: 0.000\n",
      "[801,    77] loss: 0.000\n",
      "[801,    78] loss: 0.000\n",
      "[801,    79] loss: 0.000\n",
      "[801,    80] loss: 0.000\n",
      "[801,    81] loss: 0.000\n",
      "[801,    82] loss: 0.000\n",
      "[801,    83] loss: 0.000\n",
      "[801,    84] loss: 0.000\n",
      "[801,    85] loss: 0.000\n",
      "[801,    86] loss: 0.000\n",
      "[801,    87] loss: 0.000\n",
      "[801,    88] loss: 0.000\n",
      "[801,    89] loss: 0.000\n",
      "[801,    90] loss: 0.000\n",
      "[801,    91] loss: 0.000\n",
      "[801,    92] loss: 0.000\n",
      "[801,    93] loss: 0.000\n",
      "[801,    94] loss: 0.000\n",
      "[801,    95] loss: 0.000\n",
      "[801,    96] loss: 0.000\n",
      "[801,    97] loss: 0.000\n",
      "[801,    98] loss: 0.000\n",
      "[801,    99] loss: 0.000\n",
      "[801,   100] loss: 0.000\n",
      "[801,   101] loss: 0.000\n",
      "[801,   102] loss: 0.000\n",
      "[801,   103] loss: 0.000\n",
      "[801,   104] loss: 0.000\n",
      "[801,   105] loss: 0.000\n",
      "[801,   106] loss: 0.000\n",
      "[801,   107] loss: 0.000\n",
      "[801,   108] loss: 0.000\n",
      "[1001,     1] loss: 0.000\n",
      "[1001,     2] loss: 0.000\n",
      "[1001,     3] loss: 0.000\n",
      "[1001,     4] loss: 0.000\n",
      "[1001,     5] loss: 0.000\n",
      "[1001,     6] loss: 0.000\n",
      "[1001,     7] loss: 0.000\n",
      "[1001,     8] loss: 0.000\n",
      "[1001,     9] loss: 0.000\n",
      "[1001,    10] loss: 0.000\n",
      "[1001,    11] loss: 0.000\n",
      "[1001,    12] loss: 0.000\n",
      "[1001,    13] loss: 0.000\n",
      "[1001,    14] loss: 0.000\n",
      "[1001,    15] loss: 0.000\n",
      "[1001,    16] loss: 0.000\n",
      "[1001,    17] loss: 0.000\n",
      "[1001,    18] loss: 0.000\n",
      "[1001,    19] loss: 0.000\n",
      "[1001,    20] loss: 0.000\n",
      "[1001,    21] loss: 0.000\n",
      "[1001,    22] loss: 0.000\n",
      "[1001,    23] loss: 0.000\n",
      "[1001,    24] loss: 0.000\n",
      "[1001,    25] loss: 0.000\n",
      "[1001,    26] loss: 0.000\n",
      "[1001,    27] loss: 0.000\n",
      "[1001,    28] loss: 0.000\n",
      "[1001,    29] loss: 0.000\n",
      "[1001,    30] loss: 0.000\n",
      "[1001,    31] loss: 0.000\n",
      "[1001,    32] loss: 0.000\n",
      "[1001,    33] loss: 0.000\n",
      "[1001,    34] loss: 0.000\n",
      "[1001,    35] loss: 0.000\n",
      "[1001,    36] loss: 0.000\n",
      "[1001,    37] loss: 0.000\n",
      "[1001,    38] loss: 0.000\n",
      "[1001,    39] loss: 0.000\n",
      "[1001,    40] loss: 0.000\n",
      "[1001,    41] loss: 0.000\n",
      "[1001,    42] loss: 0.000\n",
      "[1001,    43] loss: 0.000\n",
      "[1001,    44] loss: 0.000\n",
      "[1001,    45] loss: 0.000\n",
      "[1001,    46] loss: 0.000\n",
      "[1001,    47] loss: 0.000\n",
      "[1001,    48] loss: 0.000\n",
      "[1001,    49] loss: 0.000\n",
      "[1001,    50] loss: 0.000\n",
      "[1001,    51] loss: 0.000\n",
      "[1001,    52] loss: 0.000\n",
      "[1001,    53] loss: 0.000\n",
      "[1001,    54] loss: 0.000\n",
      "[1001,    55] loss: 0.000\n",
      "[1001,    56] loss: 0.000\n",
      "[1001,    57] loss: 0.000\n",
      "[1001,    58] loss: 0.000\n",
      "[1001,    59] loss: 0.000\n",
      "[1001,    60] loss: 0.000\n",
      "[1001,    61] loss: 0.000\n",
      "[1001,    62] loss: 0.000\n",
      "[1001,    63] loss: 0.000\n",
      "[1001,    64] loss: 0.000\n",
      "[1001,    65] loss: 0.000\n",
      "[1001,    66] loss: 0.000\n",
      "[1001,    67] loss: 0.000\n",
      "[1001,    68] loss: 0.000\n",
      "[1001,    69] loss: 0.000\n",
      "[1001,    70] loss: 0.000\n",
      "[1001,    71] loss: 0.000\n",
      "[1001,    72] loss: 0.000\n",
      "[1001,    73] loss: 0.000\n",
      "[1001,    74] loss: 0.000\n",
      "[1001,    75] loss: 0.000\n",
      "[1001,    76] loss: 0.000\n",
      "[1001,    77] loss: 0.000\n",
      "[1001,    78] loss: 0.000\n",
      "[1001,    79] loss: 0.000\n",
      "[1001,    80] loss: 0.000\n",
      "[1001,    81] loss: 0.000\n",
      "[1001,    82] loss: 0.000\n",
      "[1001,    83] loss: 0.000\n",
      "[1001,    84] loss: 0.000\n",
      "[1001,    85] loss: 0.000\n",
      "[1001,    86] loss: 0.000\n",
      "[1001,    87] loss: 0.000\n",
      "[1001,    88] loss: 0.000\n",
      "[1001,    89] loss: 0.000\n",
      "[1001,    90] loss: 0.000\n",
      "[1001,    91] loss: 0.000\n",
      "[1001,    92] loss: 0.000\n",
      "[1001,    93] loss: 0.000\n",
      "[1001,    94] loss: 0.000\n",
      "[1001,    95] loss: 0.000\n",
      "[1001,    96] loss: 0.000\n",
      "[1001,    97] loss: 0.000\n",
      "[1001,    98] loss: 0.000\n",
      "[1001,    99] loss: 0.000\n",
      "[1001,   100] loss: 0.000\n",
      "[1001,   101] loss: 0.000\n",
      "[1001,   102] loss: 0.000\n",
      "[1001,   103] loss: 0.000\n",
      "[1001,   104] loss: 0.000\n",
      "[1001,   105] loss: 0.000\n",
      "[1001,   106] loss: 0.000\n",
      "[1001,   107] loss: 0.000\n",
      "[1001,   108] loss: 0.000\n",
      "Finished Training\n",
      "Accuracy of the network on the test data: 24 %\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network and optimizer\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assume we have a data loader `train_dataloader` which loads our training accelerometer data\n",
    "for epoch in range(1001):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (epoch % 200) == 0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Now we will validate the model using test data\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():   # Since we're not training, we don't need to calculate the gradients\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test data: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "\n",
    "# Saving the entire model\n",
    "\n",
    "torch.save(net, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf52100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
