{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%run AbstractModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we added 2 more convolutional layers, which can extract more complex features from your accelerometer data. The number of output channels in the convolutional layers gradually increases, as it is common in many deep learning models to gradually increase the complexity and decrease the spatial size.\n",
    "\n",
    "Please note that the dimension of the input to the fully connected layer depends on the output size of your last convolutional layer. This code assumes that after 3 layers of convolution with kernel size 5 and stride 1, the sequence length is reduced to 82 (from the original 100). You may need to adjust this according to your own situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JFNet(nn.Module, SKModel):\n",
    "    def __init__(self, sequence_length):\n",
    "        super(JFNet, self).__init__()\n",
    "        # assertion\n",
    "        SKDescriptors.validate_class_type(classtype)\n",
    "        # needed definitions\n",
    "        self.classtype = classtype\n",
    "        self.sequence_length = sequence_length\n",
    "        input_num = SKDescriptors.NUM_OF_INPUTS_PER_TYPE[self.classtype]\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_num, 64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3)\n",
    "        \n",
    "        # Adjust the fully connected layer's input size based on the new sequence length after convolutions.\n",
    "        # Adjusted for sequence length = 4 after 3 conv layers with kernel size 3\n",
    "        # 10 -3 + 1 = 8 after the first layer\n",
    "        # 8 - 3 + 1 = 6 after the second layer\n",
    "        # 6 - 3 + 1 = 4 after the third layer\n",
    "        self.fc1 = nn.Linear(256 * (self.sequence_length - 6), 128)  # Adjusted for sequence length = 4 after 3 conv layers with kernel size 3\n",
    "        self.fc2 = nn.Linear(128, SKDescriptors.NUM_OF_CLASSES_PER_TYPE[self.classtype])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)  # Apply softmax to the output layer\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # All dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def skm_train(self):\n",
    "        # Instantiate the network and optimizer\n",
    "        # net = Net()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.01)\n",
    "\n",
    "        # Define the loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Assume we have a data loader `train_dataloader` which loads our training accelerometer data\n",
    "        for epoch in range(1001):  # loop over the dataset multiple times\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_dataloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if (epoch % 200) == 0:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                        (epoch + 1, i + 1, running_loss / 2000))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "        # Now we will validate the model using test data\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():   # Since we're not training, we don't need to calculate the gradients\n",
    "            for data in test_dataloader:\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the test data: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "\n",
    "        # Saving the entire model\n",
    "\n",
    "        torch.save(net, 'model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
