{"cells":[{"cell_type":"code","execution_count":2,"id":"bZLKiFqtPRNs","metadata":{"id":"bZLKiFqtPRNs"},"outputs":[],"source":["# # no need for this since it has nothing outside classes and functions\n","# print(__name__)\n","# if __name__ == \"__main__\" and hasattr(__builtins__,'__IPYTHON__') and ('google.colab' in str(get_ipython())):\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","#     %cd /content/drive/MyDrive/PressureReliefWorkArea/SummerWork/\n","#     !ls"]},{"cell_type":"code","execution_count":27,"id":"b20374ca","metadata":{"id":"b20374ca"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import torch.optim as optim\n","import numpy as np\n","\n","%run -n HelperFunctions.ipynb\n","# import ipynb\n","# from ipynb.fs.full.HelperFunctions import *"]},{"cell_type":"markdown","id":"5d837446","metadata":{"id":"5d837446"},"source":["This code loads your CSV file, splits the data into a training set and a test set, and creates a DataLoader for each. The DataLoader can be used to iterate through the data in batches, which is useful for training a neural network.\n","\n","You can replace 'yourfile.csv' with the path to your actual file. Also, note that this assumes your CSV file doesn't have a header. If it does, you might need to skip the first row."]},{"cell_type":"code","execution_count":28,"id":"b7bd7f69","metadata":{"id":"b7bd7f69"},"outputs":[],"source":["class JFSKAccelDataset(Dataset):\n","    def __init__(self, data, labels, sequence_length=10):\n","        # if(labels == None):\n","        #     self.data = data.data\n","        #     self.labels = data.labels\n","        #     self.sequence_length = data.sequence_length\n","        #     return\n","        self.data = data\n","        self.labels = labels\n","        self.sequence_length = sequence_length\n","        \n","    def group(self):\n","        self.data = [self.data[i:i+self.sequence_length] for i in range(len(self.data) - self.sequence_length + 1)]\n","        self.labels = self.labels[(int)(self.sequence_length/2) - 1 : len(self.labels) - (self.sequence_length - (int)(self.sequence_length/2))]\n","        # change to get the majority\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx].transpose(0, 1), self.labels[idx]  # Transposing the sequence and channel dimensions"]},{"cell_type":"code","execution_count":6,"id":"OBcQ2UO1K1tM","metadata":{"id":"OBcQ2UO1K1tM"},"outputs":[],"source":["class SKInputConverter:\n","    def __init__(self, dataframe, classtype, bufferpref = \"Inner\"):\n","        self.dataframe = dataframe\n","        SKDescriptors.validate_class_type(classtype)\n","        self.classtype = classtype\n","        self.bufferpref = bufferpref\n","        self.inputnum = SKDescriptors.NUM_OF_INPUTS_PER_TYPE[classtype]\n","\n","    def result(self):\n","        return self.dataframe, self.inputnum\n","\n","    def diff(self):\n","        pass\n","        return self\n","\n","    def remove_outliers(self, rem_type = None, rem_func = None, *args, **kwargs):\n","        pass\n","        return self\n","\n","    def normalize(self, norm_type = None, norm_func = None, *args, **kwargs):\n","        pass\n","        return self\n","\n","    def combine(self, comb_type = None, comb_func = None, *args, **kwargs):\n","        if comb_type != None or comb_func != None or args or kwargs:\n","            raise NotImplementedError(f\"SKInputConverter.combine() has no implemented parameters\")\n","        self.dataframe = self.dataframe.iloc[:,:self.inputnum].apply(np.linalg.norm).join(self.dataframe.iloc[:,self.inputnum:])\n","        self.dataframe.columns = pd.Index(np.arange(len(self.dataframe.columns) - self.inputnum + 1))\n","        self.inputnum = 1\n","        return self"]},{"cell_type":"code","execution_count":5,"id":"cc4ed681","metadata":{"id":"cc4ed681"},"outputs":[],"source":["class JFSKLoader:\n","    def __init__(self, file_path, sequence_length = 10, *args, **kwargs):\n","        self.sequence_length = sequence_length\n","\n","\n","        # 1. open file\n","\n","        # Gather file info\n","        # self.file_directory, self.beginning_descriptors, self.file_name, self.ending_descriptors, self.file_extension, self.specifier_values = SKFileNameHandler.read_data_file_name(file_path)\n","        self.file_directory, _, _, _, file_extension, self.specifier_values = SKFileNameHandler.read_data_file_name(file_path)\n","        classification_type = self.specifier_values[SKDescriptors.CLASSIFICATION_TYPE_FS]\n","        input_num = SKDescriptors.NUM_OF_INPUTS_PER_TYPE[classification_type]\n","\n","        match file_extension:\n","            case \".csv\":\n","                dataframe = pd.read_csv(file_path)\n","                # code test file: Data/Week 1/Left then Right/Processed/Type3-Freq10-Labeled_Motion-sessions_2023-08-26_17-25-54.csv\n","                # classifier training file: Data/COMBINED_Type3-Freq10-Labeled_Motion-sessions_23-24_Fall.csv\n","            case _:\n","                raise NotImplementedError(f\"JFSKLoader is not equipped to open {file_extension} files.\")\n","            \n","        # # This combines the input names with the class names\n","        #     # example: ['x', 'y', 'z', 'Forward Lean', 'Left Lean', 'Right Lean', 'Pushup', 'Other']\n","        #     # I use the * operator in conjunction with Python 3.5+'s \"Additional Unpacking Generalizations\"\n","        #         # to unpack two list-likes and combine them into one list\n","        # dataframe.columns = pd.Index([*SKDescriptors.INPUT_NAMES[1], *(str(c) for c in SKDescriptors.CTS_PER_TYPE[classification_type])])\n","\n","\n","        # 2. split dataset into data and labels\n","\n","        # Get data and labels from dataframe\n","        data = dataframe.iloc[:, :input_num].to_numpy()  # x, y, z data\n","        labels = dataframe.iloc[:, input_num:].to_numpy()  # labels\n","\n","\n","        # 3. make adjustments related to the data\n","        # THIS is where we would use SKInputConverter\n","\n","\n","        # 4. group\n","        self.g_dataset = JFSKAccelDataset(data, labels, sequence_length)\n","        self.g_dataset.group()\n","\n","\n","        # 5. make adjustments related to the labels\n","        # if repress_classes:\n","        if args and args[0]:\n","            if len(args) > 1:\n","                self.repress_classes(*(args[1:]), **kwargs)\n","            else:\n","                self.repress_classes(**kwargs)\n","        elif kwargs.get(\"repress_classes\", False):\n","            self.repress_classes(*args, **kwargs)\n","\n","\n","        # 6. randomize\n","        data_train, data_test, labels_train, labels_test = train_test_split(self.g_dataset.data, self.g_dataset.labels, test_size=0.2, random_state=42)\n","\n","\n","        # 7. create dataloader\n","\n","\n","        # Convert data to tensors\n","        data_train = torch.tensor(np.array(data_train), dtype=torch.float32)  \n","        data_test = torch.tensor(np.array(data_test), dtype=torch.float32)\n","\n","        # Convert labels to tensors and get max index (assuming one-hot encoding)\n","        labels_train = torch.argmax(torch.tensor(np.array(labels_train), dtype=torch.float32), dim=1)\n","        labels_test = torch.argmax(torch.tensor(np.array(labels_test), dtype=torch.float32), dim=1)\n","\n","        # Create data loaders\n","        train_dataset = JFSKAccelDataset(data_train, labels_train, self.sequence_length)\n","        test_dataset = JFSKAccelDataset(data_test, labels_test, self.sequence_length)\n","\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","        self.test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","\n","\n","\n","    # target_classes = [\"Other\", \"Stationary\"], rep_format = 'tag', rep_func = np.mean, skip_repressed = True, *args, **kwargs\n","    def repress_classes(self, target_classes = None, rep_format = 'tag', rep_func = np.mean, skip_repressed = True, *args, **kwargs):\n","        classification_type = self.specifier_values[SKDescriptors.CLASSIFICATION_TYPE_FS]\n","        \n","        # NOTE: the logic here only works for one-hot vectors\n","        if not SKDescriptors.NUM_OF_OUTPUTS_PER_TYPE[classification_type] == 1:\n","            raise NotImplementedError(f\"JFSKLoader.repress_classes() with repress_stationary=True may not be equipped to fairly sample stationary data for classification types with outputs that are not one-hot vectors.\")\n","\n","        if not callable(rep_func):\n","            raise NotImplementedError(f\"JFSKLoader.repress_classes()'s rep_func must currently be a function\\\n","                                      \\n\\tIf you want more complex logic where you'd test for a string or something, feel free to alter the code\")\n","\n","        if target_classes is None:\n","            target_classes = [SKDescriptors.OTHER_TAG, SKDescriptors.STATIONARY_TAG]\n","            rep_format = 'tag'\n","\n","        \n","\n","        temp_list = []\n","        match rep_format:\n","            case 'str' | 'cts':\n","                for i, cts in enumerate(SKDescriptors.CTS_PER_TYPE[classification_type]):\n","                    if cts in target_classes:\n","                        temp_list.append(i)\n","\n","                if len(temp_list) != len(target_classes):\n","                    print(f\"\\nWARNING: The following target_classes were not kept as they do not correspond to classification_type '{classification_type}':\\n\")\n","                    for cts in target_classes:\n","                        if not any(cts == SKDescriptors.CTS_PER_TYPE[classification_type][i] for i in temp_list):\n","                            print(f\"\\t{cts}\\n\")\n","\n","            case 'tag':\n","                for i, cts in enumerate(SKDescriptors.CTS_PER_TYPE[classification_type]):\n","                    for tag in target_classes:\n","                        if tag in cts:\n","                            temp_list.append(i)\n","\n","            case 'num' | 'int':\n","                forwarn_active = False\n","                for i in target_classes:\n","                    if i in np.arange(SKDescriptors.NUM_OF_CLASSES_PER_TYPE[classification_type]):\n","                        temp_list.append(i)\n","                        continue\n","\n","                    if not forwarn_active:\n","                        print(f\"\\nWARNING: The following target_classes were not kept as they do not correspond to classification_type '{classification_type}':\\n\")\n","                        forwarn_active = True\n","                    print(f\"\\t{i}\\n\")\n","                \n","            case _ :\n","                raise AssertionError(\"You can only pass 'str', 'tag', 'cts', 'num', or 'int' into the rep_format parameter of JFSKLoader.repress_classes()\\\n","                                     \\n\\t\\t'str' is used when you want to pass the class names (as strings) of classes it should repress\\\n","                                     \\n\\t\\t'tag' is used when you want to pass the class tags (as strings) it should repress\\\n","                                     \\n\\t\\t'cts' is used when you want to pass the classes (as ClassTagSets) it should repress\\\n","                                     \\n\\t\\t'int' and 'num' are used when you want to pass the indices of the classes it should repress\")\n","            \n","        target_classes = temp_list\n","\n","\n","        # # my population sample randomizer from STAT 2113\n","        # for i in range(SAMPLE_SIZE):\n","        #     choice_index = random.randint(1, unchosen_len) - 1\n","        #     choice = unchosen[choice_index]\n","        #     unchosen.pop(choice_index)\n","        #     unchosen_len -= 1\n","        #     choices_dict[choice] = choices_dict.setdefault(choice, 0)\n","        #     choices_dict[choice] += 1\n","        # choices_list = sorted(choices_dict)\n","\n","        # we should not iterate over a dataframe\n","        # if class_index != -1:\n","        # check the counts of every class\n","        # this first removes the input columns;\n","            # then it removes the Stationary column;\n","            # then it counts each remaining columns' counts of '1' (everything inside .apply());\n","                # inside .apply(), we count how many of each number we have;\n","                # then we change it to a zip, then a dict;\n","                # then we take the count from key 1,\n","                # and if it doesn't have a 1 key, we return 0;\n","            # then it converts it to a numpy array;\n","            # then it takes the mean of the counts;\n","            # then it turns this into an int\n","\n","        # use this if we want the mean, but take into account the fact that some values may overpower the others\n","        if skip_repressed:\n","            accounted = np.delete(self.g_dataset.labels, target_classes, axis=1)\n","        else:\n","            accounted = self.g_dataset.labels\n","        adjustment_height = int(rep_func(np.apply_along_axis(lambda x: dict(zip(*np.unique(x, return_counts = True))).get(1, 0), 1, accounted), *args, **kwargs))\n","        # avg_count = int(dataframe[dataframe.columns[3 : len(dataframe.columns) - 2]].apply(class_counter).to_numpy().mean())\n","        \n","        NOT REIMPLEMENTED FURTHER YET -- 6/15/2024\n","\n","        # class_counts = {}\n","        # for i in dataframe.columns[input_num : ]:\n","        #     if i == SKDescriptors.STATIONARY_CLASS:\n","        #         stationary_rows = dataframe[dataframe[i] == 1]\n","        #         continue\n","        #     class_counts[i] = len(dataframe[dataframe[i] == 1])\n","        stationary_rows = dataframe[dataframe[len(dataframe.columns) - 2] == 1]\n","        other_rows = dataframe[dataframe[len(dataframe.columns) - 1] == 1]\n","        print()\n","        print(avg_count)\n","        print(\"statlen: \" + str(len(stationary_rows)))\n","        print(\"otherlen: \" + str(len(other_rows)))\n","        # this is the line doing the actual randomization\n","        sample_rows = stationary_rows.sample(avg_count, random_state=42)\n","        print(len(sample_rows))\n","        dataframe = dataframe.drop(stationary_rows.drop(sample_rows.index).index)\n","        print(len(dataframe))\n","        # this is the line doing the actual randomization\n","        sample_rows = other_rows.sample(avg_count, random_state=42)\n","        print(len(sample_rows))\n","        dataframe = dataframe.drop(other_rows.drop(sample_rows.index).index)\n","        print(len(dataframe))\n","\n","\n","        # want min instead?\n","        avg_count = int(dataframe[dataframe.columns[input_num : ]].apply(JFSKLoader.count_instances).to_numpy().min())\n","\n","        # median\n","        avg_count = int(np.median(dataframe[dataframe.columns[3 : ]].apply(lambda x: skcounter(x)).to_numpy()))\n","\n","        for i in dataframe.columns[input_num : ]:\n","            class_i_rows = dataframe[dataframe[i] == 1]\n","            # this is the line doing the actual randomization\n","            sample_rows = class_i_rows.sample(min(len(class_i_rows), avg_count), random_state=42)\n","            # print(len(sample_rows))\n","            dataframe = dataframe.drop(class_i_rows.drop(sample_rows.index).index)\n","            # print(len(dataframe))\n","\n","\n","\n","        avg_count = int(dataframe[dataframe.columns[input_num : ]].drop(SKDescriptors.STATIONARY_CLASS, axis=1).apply(lambda x: dict(zip(np.unique(x, return_counts = True))).get(1, 0)).to_numpy().mean())\n","        # class_counts = {}\n","        # for i in dataframe.columns[input_num : ]:\n","        #     if i == SKDescriptors.STATIONARY_CLASS:\n","        #         stationary_rows = dataframe[dataframe[i] == 1]\n","        #         continue\n","        #     class_counts[i] = len(dataframe[dataframe[i] == 1])\n","        stationary_rows = dataframe[dataframe[SKDescriptors.STATIONARY_CLASS] == 1]\n","        # this is the line doing the actual randomization\n","        sample_rows = stationary_rows.sample(avg_count, random_state=42)\n","        dataframe = dataframe.drop(stationary_rows.drop(sample_rows.index).index)\n","\n","\n","\n","\n","\n","    def count_instances(series):\n","        return dict(zip(*np.unique(series, return_counts = True))).get(1, 0)\n","\n","        # ci_arr = np.unique(series, return_counts = True)\n","        # return dict(zip(ci_arr[0], ci_arr[1])).get(1, 0)\n","    \n","        # print(skarr)\n","        # find_index = np.where(skarr[0], True, False)\n","        # for i in range(len(find_index)):\n","        #     if find_index[i]: ret_valsk = skarr[1][i]\n","        # print(ret_dict)\n","        # ret_valsk = ret_dict.get(1, 0)\n","        # print(ret_valsk)\n","        # # print(dataframe)\n","        # return ret_valsk\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":5}
