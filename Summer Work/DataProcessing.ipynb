{"cells":[{"cell_type":"code","execution_count":null,"id":"bZLKiFqtPRNs","metadata":{"id":"bZLKiFqtPRNs"},"outputs":[],"source":["# # no need for this since it has nothing outside classes and functions\n","# print(__name__)\n","# if __name__ == \"__main__\" and hasattr(__builtins__,'__IPYTHON__') and ('google.colab' in str(get_ipython())):\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","#     %cd /content/drive/MyDrive/PressureReliefWorkArea/SummerWork/\n","#     !ls"]},{"cell_type":"code","execution_count":null,"id":"b20374ca","metadata":{"id":"b20374ca"},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import torch.optim as optim\n","import numpy as np\n","from sklearn.utils.random import sample_without_replacement\n","# from collections.abc import Iterable # , Sequence\n","from copy import deepcopy\n","# from inspect import signature\n","# from inspect import getmembers, ismethod\n","from itertools import chain\n","\n","%run -n HelperFunctions.ipynb\n","# import ipynb\n","# from ipynb.fs.full.HelperFunctions import *"]},{"cell_type":"markdown","id":"5d837446","metadata":{"id":"5d837446"},"source":["This code loads your CSV file, splits the data into a training set and a test set, and creates a DataLoader for each. The DataLoader can be used to iterate through the data in batches, which is useful for training a neural network.\n","\n","You can replace 'yourfile.csv' with the path to your actual file. Also, note that this assumes your CSV file doesn't have a header. If it does, you might need to skip the first row."]},{"cell_type":"code","execution_count":null,"id":"b7bd7f69","metadata":{"id":"b7bd7f69"},"outputs":[],"source":["class JFAccelDataset(Dataset):\n","    def __init__(self, data, labels, sequence_length=None):\n","        # if(labels == None):\n","        #     self.data = data.data\n","        #     self.labels = data.labels\n","        #     self.sequence_length = data.sequence_length\n","        #     return\n","        self.data = data\n","        self.labels = labels\n","        self.sequence_length = sequence_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx].transpose(0, 1), self.labels[idx]  # Transposing the sequence and channel dimensions\n","        \n","        \n","    def group(self):\n","        self.data = [self.data[i:i+self.sequence_length] for i in range(len(self.data) - self.sequence_length + 1)]\n","        self.labels = self.labels[(int)(self.sequence_length/2) - 1 : len(self.labels) - (self.sequence_length - (int)(self.sequence_length/2))]\n","        # change to get the majority"]},{"cell_type":"code","execution_count":null,"id":"OBcQ2UO1K1tM","metadata":{"id":"OBcQ2UO1K1tM"},"outputs":[],"source":["class SKDatasetHandler:\n","\n","    NDARRAY_JFAD_DTYPE = (\"NDArray JFAccelDataset\", None)\n","    NDARRAY_JFAD_PAIR_DTYPE = (\"NDArray JFAccelDataset Pair\", 2)\n","    TENSOR_JFAD_PAIR_DTYPE = (\"Tensor JFAccelDataset Pair\", 2)\n","    VALID_DATASET_DATATYPES = {\n","        NDARRAY_JFAD_DTYPE,\n","        NDARRAY_JFAD_PAIR_DTYPE,\n","        TENSOR_JFAD_PAIR_DTYPE\n","    }\n","\n","\n","    # def format_scalar\n","    \n","\n","    def get_state_slice(state, index):\n","        if state[\"ds_format\"][1] is None:\n","            return state\n","        if index not in range(state[\"ds_format\"][1]):\n","            raise IndexError(f\"SKDatasetHandler.get_state_slice() could not find index {index} for state {state['description']}.\")\n","        \n","        ret_state = {}\n","        for k, v in state.items():\n","            if k in (\"ds_format\", \"description\") or type(v) is not tuple:\n","                ret_state[k] = v\n","            else:\n","                ret_state[k] = v[index]\n","        return ret_state\n","    \n","\n","    # ds_format = tuple[Sequence[torch.Tensor], Sequence[torch.Tensor]]\n","    def __init__(self, datasets, classification, ds_format = NDARRAY_JFAD_PAIR_DTYPE, is_argmax_format = False, bufferpref = \"Inner\", *, description):\n","        SKDescriptors.validate_class_type(classification)\n","        self.ds_format          = ds_format # always has only one value\n","        self.description        = description # always has only one value\n","        # self.adj_test           = True # False # always has only one value, should be considered for removal\n","        self.datasets           = datasets\n","        self.classification     = classification\n","        self.bufferpref         = bufferpref\n","        self.is_argmax_format   = is_argmax_format\n","        self.inputnum           = SKDescriptors.NUM_OF_INPUTS_PER_TYPE[classification]\n","        self.dataset_types      = (\"Train\", \"Test\") if ds_format == SKDatasetHandler.NDARRAY_JFAD_PAIR_DTYPE else \"Full\"\n","        self.__log              = []\n","        self.__checkpoints      = []\n","        self.log_state()\n","        self.__fail_counter     = 0\n","\n","\n","    def __str__(self):\n","        return str(self.description)\n","    \n","\n","    def __len__(self):\n","        return len(self.__log)\n","    \n","\n","    def has_valid_description(self):\n","        for s in self.__log:\n","            if self.description == s[\"description\"]:\n","                return False\n","        return True\n","\n","\n","    def get_state(self, index = None, feedback = True):\n","        if index is None:\n","            vals = {}\n","            for name, value in vars(self).items():\n","                if not name.startswith('_'):\n","                    vals[name] = value\n","            # vals = tuple(vals)\n","            if feedback:\n","                print(f\"Read state as {vals}\")\n","            return vals\n","        if index not in range(len(self.__log)):\n","            raise IndexError(f\"{index} is not a valid index for log of size {len(self.__log)}.\")\n","        if feedback:\n","            print(f\"Read state {index} as {self.__log[index]}\")\n","        return self.__log[index]\n","    \n","\n","    def log_state(self, mark_cp = True, return_index = False, feedback = True):\n","        if not self.has_valid_description():\n","            print(\"WARNING: attempting to log a state without changing the description could result in not being able to tell datasets apart.\")\n","            self.description = str(self.__fail_counter)\n","            self.__fail_counter += 1\n","            print(f\"\\tLogging state with description '{self.description}'\")\n","        vals = self.get_state(None, False)\n","        # if type(vals[\"datasets\"][0]) != JFAccelDataset:\n","        #     self.apply(lambda ds: JFAccelDataset(ds, batch_size=64, shuffle=True))\n","        self.__log.append(vals)\n","        if mark_cp:\n","            self.__checkpoints.append(len(self.__log) - 1)\n","        if feedback:\n","            print(f\"Logged state {len(self.__log) - 1} as {vals}\")\n","        if return_index:\n","            return len(self.__log) - 1\n","\n","    \n","    def recall_state(self, index, mark_cp = True, feedback = True, *, __from_restore = False):\n","        double_restore = __from_restore\n","        print(\"loggggggg\")\n","        print(self.__log[index])\n","        for name, current_val in vars(self).items():\n","            if not name.startswith('_'):\n","                value = self.__log[index][name]\n","                if not current_val == value:\n","                    double_restore = False\n","                    setattr(self, name, value)\n","        if double_restore:\n","            self.restore_state(feedback)\n","            return\n","        if mark_cp:\n","            self.__checkpoints.append(index)\n","        if feedback:\n","            print(f\"Recalled state {index} as {self.__log[index]}\")\n","\n","\n","    def restore_state(self, feedback = True):\n","        index = 0\n","        if self.__checkpoints:\n","            index = self.__checkpoints.pop()\n","        self.recall_state(index, True, feedback, __from_restore = True)\n","\n","\n","    def reset_state(self, feedback = True):\n","        self.recall_state(0, False, feedback)\n","        self.__checkpoints = []\n","\n","\n","# , *args, **kwargs\n","    def apply(self, func, copy_datasets = None, is_adj_func = False, save = None, keep_format = None, *args, **kwargs):\n","        # args = []\n","        # kwargs = {}\n","        # ft_info = iterable_info(func_tuple)\n","        # if not ft_info[0]:\n","        #     raise TypeError()\n","        # func_tuple = ft_info[0]\n","        # func = func_tuple[0]\n","        # if ft_info[2] > 1:\n","        #     args = func_tuple[1]\n","        # if ft_info[2] > 2:\n","        #     kwargs = func_tuple[2]\n","            \n","            # (not (func in (SKDatasetHandler.save_data_and_labels, SKDatasetHandler.get_inverse_and_counts)) and func in list(zip(*getmembers(self, ismethod)))[1])\n","        if not callable(func) or func == self.apply:\n","            raise TypeError(f\"ERROR: using {func} within SKDatasetHandler.apply() as the function will likely yield undesired results.\\\n","                            \\n\\tIf you are trying to combine two datasets in some fashion, try list(zip(ds1, ds2)) as the input dataset and access them\\\n","                            \\n\\tby using lambda_arg[0] and lambda_arg[1].\\\n","                            \\n\\tIf you are trying to use one of SKDatasetHandler's functions, set the object's state based on the dataset you want\\\n","                            \\n\\tto apply it to and then call that function without using SKDatasetHandler.apply(). SKDatasetHandler.apply() is for\\\n","                            \\n\\tfunctions from outside the class that take in a dataset as their first parameter\")\n","        skip_test = False # is_adj_func and not self.adj_test\n","        if copy_datasets is None:\n","            datasets = deepcopy(self.datasets)\n","            if save is None:\n","                save = True\n","        else:\n","            datasets = deepcopy(copy_datasets)\n","            if save is None:\n","                save = False\n","        if keep_format is None:\n","            keep_format = save\n","        else:\n","            keep_format = save or keep_format\n","\n","        if type(datasets) is tuple or self.ds_format[1] > 1:\n","            all_eq = True\n","            first = None\n","            result = []\n","            for ds, dst in zip(datasets, self.dataset_types):\n","                if dst == \"Test\" and skip_test:\n","                    result.append(ds)\n","                    continue\n","                temp = func(ds, *args, **kwargs)\n","                result.append(temp)\n","\n","                if first is None:\n","                    first = temp\n","                if type(temp) is not int:\n","                    all_eq = False\n","                    # print(\"boo\")\n","                elif all_eq:\n","                    all_eq = all_eq and first == temp\n","                    # print(f\"yay -- all_eq: {all_eq}, temp: {temp}\")\n","                \n","            if all_eq and not keep_format and first is not None:\n","                result = first\n","                # print(f\"yay: {result}\")\n","            else:\n","                result = tuple(result)\n","                # print(\"boo\")\n","        else:\n","            assert not skip_test # don't be adjusting the whole dataset, only the training data\n","            result = func(datasets, *args, **kwargs)\n","        \n","        if save:\n","            self.datasets = result\n","        else:\n","            return result\n","\n","    def apply_all(self, funcs_plus_args):\n","        # funcs_plus_args is a list of tuples\n","            # Each tuple holds a function to apply, how to apply it, and arguments to pass to it\n","        # Below we explain the different parts, but this is what it might look like (this would not be 100% valid)\n","            # [(combine), (True, None, lambda x: x.bufferpref = \"Outer\", [], {}), (False, ((2, 0, 3), (3, 1, 0)), np.mean)]\n","        # The default format of each tuple is (use_self, flow_info, func, args, kwargs)\n","            # purposes:\n","                # use_self is a bool saying whether to use self as the first arg or to leave it off\n","                # flow_info is a tuple (or None) that holds information about what outputs from other funcs should be passed into it\n","                # func is the function to apply\n","                # args is a list to be entered into the function as *args\n","                # kwargs is a dict to be entered into the function as **kwargs\n","            # everything but func is not always necessary\n","            # on flow_info:\n","                # if it is None, it does not use other funcs' outputs\n","                # if it is a tuple, it will contain other tuples that each correspond to one argument\n","                # the format for if it is a tuple is (arg_index, source_index, return_index)\n","                    # all three are ints\n","                    # purposes:\n","                        # arg_index tells which parameter to pass the output into\n","                        # source_index tells which tuple in funcs_plus_args represents where to get the output\n","                        # return_index tells which output of the func it should pull from\n","                    # arg_index:\n","                        # must be unique\n","                        # causes argument with that position passed in args to be ignored\n","                            # if that position is represented in kwargs, that should be ignored too\n","                    # return_index is not necessary since most functions only return one output\n","        outputs = []\n","\n","        # not all of the mentioned quality of life logic is here;\n","            # do not remove any arguments, and don't expect flow_info to override anything from kwargs\n","        for fpa in funcs_plus_args:\n","            assert len(fpa) == 5\n","            args = fpa[3]\n","            # if len(fpa) > 1 and fpa[1] is not None:\n","                # firstarg = fpa[1]\n","            for t in fpa[1]:\n","                if len(t) == 2:\n","                    i, j, = t\n","                    # k = 0\n","                    assert i <= len(args)\n","                    args[i] = outputs[j]\n","                else:\n","                    i, j, k = t\n","                    assert i <= len(args)\n","                    args[i] = outputs[j][k]\n","            if fpa[0]:\n","                args = [self] + args\n","            output = fpa[2](args, **fpa[4])\n","            outputs.append(output)\n","        return output\n","\n","            # if len(fpa) == 1:\n","            #     fpa[0](firstarg)\n","            # else:\n","            #     fpa[0](firstarg, *fpa[1 : ])\n","\n","\n","\n","    def save_data_and_labels(dataset_data_labels_list):\n","        dataset = deepcopy(dataset_data_labels_list[0])\n","        dataset.data = dataset_data_labels_list[1]\n","        dataset.labels = dataset_data_labels_list[2]\n","        return dataset\n","\n","    def __get_inverse_and_counts_col(col):\n","        # call_unique = lambda col: np.unique(col, return_inverse = True, return_counts = True)\n","        # invert_mid = lambda info: (info[0], invert(info[1]), info[2])\n","        # reorder_mid = lambda info: (info[0], tuple(info[1][k] for k in info[0]), info[2])\n","        # format_unique = lambda col: reorder_mid(invert_mid(call_unique(col)))\n","        # get_inverse_and_counts_col = lambda col: dict([(a, (b, c)) for a, b, c in zip(*format_unique(col))])\n","        # call_unique\n","        col_info = np.unique(col, return_inverse = True, return_counts = True)\n","        # print(col_info)\n","        # invert_mid\n","        col_info = (col_info[0], invert(col_info[1]), col_info[2])\n","        # print(col_info)\n","        # reorder_mid\n","        col_info = (col_info[0], tuple(col_info[1][k] for k in col_info[0]), col_info[2])\n","        # print(col_info)\n","        # package as a dict\n","        retval = dict([(a, (b, c)) for a, b, c in zip(*col_info)])\n","        # for k, v in retval.items():\n","        #     print(f\"{k}: {v[1]}\")\n","        # print(retval.get(1, (0, 0)))\n","        return retval\n","\n","    def get_inverse_and_counts(dataset, is_argmax_format):\n","        # get_counts = cache(lambda col: dict(zip(*np.unique(col, return_counts = True))))\n","\n","        \n","        # print([n for n in call_unique(repressed_dataset[0].labels)])\n","        # ye = invert_mid(call_unique(repressed_dataset[0].labels))\n","        # print(call_unique(repressed_dataset[0].labels))\n","        # print(ye)\n","        # print(ye[1].keys())\n","        # # print([n for n in reorder_mid(ye)[1]])\n","        # for k in ye[0]:\n","        #     print(f\"k = {k}\")\n","        #     print(ye[1][k])\n","        # print([ye[1][k] for k in ye[0]])\n","        # tuple(info[1][k] for k in info[0])\n","        # print(list(zip(*format_unique(repressed_dataset[0].labels))))\n","        # print([n for n in get_inverse_and_counts(repressed_dataset[0].labels)])\n","        # raise Exception(\"yippee\")\n","\n","        if is_argmax_format:\n","            # accounted = set(range(SKDescriptors.NUM_OF_CLASSES_PER_TYPE[self.classification])) - set(target_classes * int(skip_repressed))\n","            \n","            # inverse_and_counts = self.apply(lambda ds: get_inverse_and_counts(ds.labels), repressed_dataset, True)\n","            return SKDatasetHandler.__get_inverse_and_counts_col(dataset.labels)\n","\n","            # no the class_indices and class_counts lines cannot be changed to the corresponding lines in the else block.\n","                # down there we utilize accounted before the lines; here we only use it in the lines.\n","                # also, accounted is almost completely different between the two;\n","                # here it holds indices, down there it holds a dataset\n","            # class_indices = self.apply(lambda ds: [ds.get(v, 0)[0] for v in repressed_dataset], inverse_and_counts, True)\n","            # class_counts = self.apply(lambda ds: [ds.get(v, 0)[1] for v in accounted], inverse_and_counts, True)\n","        else:\n","            # accounted = self.apply(lambda ds: np.delete(ds.labels, target_classes * int(skip_repressed), axis=1), repressed_dataset, True)\n","            # accounted = self.apply(lambda ds: np.delete(ds.labels, target_classes, axis=1) if skip_repressed else ds.labels, repressed_dataset, True)\n","            \n","            # inverse_and_counts = self.apply(lambda ds: \\\n","            #                                 dict(zip(ds, np.apply_along_axis(lambda col: get_inverse_and_counts(col).get(1, (0, 0)), 0, ds))), \\\n","            #                                 repressed_dataset, True)\n","\n","            # a3result = np.apply_along_axis(SKDatasetHandler.__get_inverse_and_counts_col, 0, dataset.labels)\n","            # inv_and_counts = {}\n","            # for i, d in enumerate(a3result):\n","            #     inv_and_counts[i] = d.get(1, ([], 0))\n","            \n","            # print(inv_and_counts)\n","            # print(dataset.labels)\n","            return dict((i, d.get(1, ([], 0))) for i, d in enumerate(np.apply_along_axis(SKDatasetHandler.__get_inverse_and_counts_col, 0, dataset.labels)))\n","        #inv_and_counts\n","        # dict(zip(dataset.labels, inv_and_counts))\n","\n","            # class_indices = self.apply(lambda ds: [b for b, c in ds.values()], inverse_and_counts, True)\n","            # class_counts = self.apply(lambda ds: [bc[1] for a, bc in ds], inverse_and_counts, True)\n","\n","\n","\n","    def result(self):\n","        return self.datasets, self.inputnum\n","\n","    def diff(self, *args, **kwargs):\n","        pass\n","        return self\n","\n","    def remove_outliers(self, rem_type = None, rem_func = None, *args, **kwargs):\n","        pass\n","        return self\n","\n","    def normalize(self, norm_type = None, norm_func = None, *args, **kwargs):\n","        pass\n","        return self\n","\n","    def combine(self, comb_type = None, comb_func = None, *args, **kwargs):\n","        if comb_type is not None or comb_func is not None or args or kwargs:\n","            raise NotImplementedError(f\"SKInputConverter.combine() has no implemented parameters\")\n","        self.dataframe = self.dataframe.iloc[:,:self.inputnum].apply(np.linalg.norm).join(self.dataframe.iloc[:,self.inputnum:])\n","        self.dataframe.columns = pd.Index(np.arange(len(self.dataframe.columns) - self.inputnum + 1))\n","        self.inputnum = 1\n","        return self\n","\n","\n","\n","    # dataset: JFSKAccelDataset, /, \n","    # target_classes = [\"Other\", \"Stationary\"], rep_format = 'str', rep_func = np.mean, skip_repressed = True, apply_to_all = False, *args, **kwargs\n","    def repress_classes(self, target_classes = None, rep_format = 'str', rep_func = np.mean, skip_repressed = True, apply_to_all = False, *args, **kwargs):\n","        # NOTE: the logic here only works for one-hot vectors\n","        if not SKDescriptors.NUM_OF_OUTPUTS_PER_TYPE[self.classification] == 1:\n","            raise NotImplementedError(f\"JFSKLoader.repress_classes() with repress_stationary=True may not be equipped to fairly sample stationary data for classification types with outputs that are not one-hot vectors.\")\n","\n","        if not callable(rep_func):\n","            raise NotImplementedError(f\"JFSKLoader.repress_classes()'s rep_func must currently be a callable (function)\\\n","                                      \\n\\tIf you want more complex logic where you'd test for a string or something, feel free to alter the code\")\n","        \n","        if target_classes is None:\n","            target_classes = [SKDescriptors.OTHER_TAG, SKDescriptors.STATIONARY_TAG]\n","            rep_format = 'tag'\n","        target_classes = SKDescriptors.format_classes(self.classification, target_classes, rep_format)\n","        if not len(target_classes) and not apply_to_all:\n","            return\n","\n","        # if self.ds_format not in SKDatasetHandler.VALID_DATASET_DATATYPES:\n","        #     raise ValueError(f\"'{self.ds_format}' is not currently a valid dataset type for SKDatasetHandler.\")\n","        repressed_dataset = self.apply(lambda ds: JFAccelDataset(deepcopy(ds.data), deepcopy(ds.labels), ds.sequence_length), None, True, False)\n","        all_classes = range(SKDescriptors.NUM_OF_CLASSES_PER_TYPE[self.classification])\n","\n","        adjusted = all_classes if apply_to_all else target_classes\n","        inverse_and_counts = self.apply(SKDatasetHandler.get_inverse_and_counts, repressed_dataset, True, False, True, self.is_argmax_format)\n","        accounted = [c for c in all_classes if c not in (target_classes * int(skip_repressed))]\n","        class_indices = self.apply(lambda ds: [ds.get(v, (0, 0))[0] for v in all_classes], inverse_and_counts, True)\n","        class_counts = self.apply(lambda ds: [ds.get(v, (0, 0))[1] for v in accounted], inverse_and_counts, True)\n","        # print(\"\\n\\n\\n\\nAdjustment height:\")\n","        adjustment_height = self.apply(lambda row: int(rep_func(row, *args, **kwargs)), class_counts, True)\n","        # print(\"\\n\\n\\n\\n\\n\")\n","        take_del_sample = lambda lz_col_ah: sample_without_replacement(len(lz_col_ah[0]), max(0, len(lz_col_ah[0]) - lz_col_ah[1]), random_state = 42)\n","        # get_class_i_rows_indexed = lambda ds, i: np.nonzero(np.transpose(ds.labels)[i])[0]\n","        apply_to_nonempty = lambda lz_ds: list(np.array(lz_ds[0])[lz_ds[1]] if lz_ds[1].size > 0 else lz_ds[1])\n","        delete_y_from_x_data = lambda lz_ds: np.delete(lz_ds[0].data, lz_ds[1], axis = 0)\n","        delete_y_from_x_labels = lambda lz_ds: np.delete(lz_ds[0].labels, lz_ds[1], axis = 0)\n","\n","        # apply_to_nonempty = lambda drni_ds, ciri_ds: [ciri_ds[ds] if ds.size > 0 else ds for ds in drni_ds]\n","        # apply_to_nonempty = lambda drni_ds, ciri_ds: ciri_ds[drni_ds] if drni_ds.size > 0 else drni_ds\n","        # delete_y_from_x_data = lambda x, y: np.delete(x.data, y, axis = 0)\n","        # delete_y_from_x_labels = lambda x, y: np.delete(x.labels, y, axis = 0)\n","        # apply_del_data = lambda del_ds: self.apply(repressed_dataset, delete_y_from_x_data, True, False, del_ds)\n","        # apply_del_labels = lambda del_ds: self.apply(repressed_dataset, delete_y_from_x_labels, True, False, del_ds)\n","\n","        del_rows_indexed = []\n","        for i in adjusted:\n","            # class_i_rows_indexed = self.apply(, repressed_dataset, True)\n","            class_i_rows_indexed = self.apply(lambda row: row[i], class_indices, True)\n","            i_del_rows_not_indexed = self.apply(take_del_sample, list(zip(class_i_rows_indexed, adjustment_height)), True, False, True)\n","            # if self.apply(lambda ds: ds.size, True, del_rows_not_indexed) > 0:\n","            #     del_rows_indexed = class_i_rows_indexed[del_rows_not_indexed]\n","            # else:\n","            #     del_rows_indexed = del_rows_not_indexed\n","            # get_del_rows_indexed = lambda ciri_ds: self.apply(del_rows_not_indexed, apply_to_nonempty, True, False, ciri_ds)\n","            # del_rows_indexed = self.apply(class_i_rows_indexed, get_del_rows_indexed, True)\n","            i_del_rows_indexed = self.apply(apply_to_nonempty, list(zip(class_i_rows_indexed, i_del_rows_not_indexed)), True)\n","            del_rows_indexed += i_del_rows_indexed\n","\n","\n","        newdata = self.apply(delete_y_from_x_data, list(zip(repressed_dataset, del_rows_indexed)), True)\n","        newlabels = self.apply(delete_y_from_x_labels, list(zip(repressed_dataset, del_rows_indexed)), True)\n","        # print(self.apply(lambda ds: np.shape(ds.data), self.datasets, save = False))\n","        # print(self.apply(np.shape, newdata, save = False))\n","        self.apply(SKDatasetHandler.save_data_and_labels, list(zip(repressed_dataset, newdata, newlabels)), True, True, True)"]},{"cell_type":"code","execution_count":null,"id":"cc4ed681","metadata":{"id":"cc4ed681"},"outputs":[],"source":["class JFSKLoader:\n","    def __init__(self, file_path, sequence_length, repress_classes = True, feedback = None, *args, **kwargs):\n","        if feedback is None:\n","            self.feedback = False\n","            feedback = True\n","        else:\n","            self.feedback = feedback\n","        # 1. open file\n","\n","        # Gather file info\n","        # self.file_directory, self.beginning_descriptors, self.file_name, self.ending_descriptors, self.file_extension, self.specifier_values = SKFileNameHandler.read_data_file_name(file_path)\n","        file_info, self.specifier_values = SKFileNameHandler.read_data_file_name(file_path, feedback)\n","        file_extension = file_info[\"File Extension\"]\n","        classification_type = self.specifier_values[SKDescriptors.CLASSIFICATION_TYPE_FS]\n","        input_num = SKDescriptors.NUM_OF_INPUTS_PER_TYPE[classification_type]\n","\n","        match file_extension:\n","            case \".csv\":\n","                dataframe = pd.read_csv(file_path)\n","                # code test file: Data/Week 1/Left then Right/Processed/Type3-Freq10-Labeled_Motion-sessions_2023-08-26_17-25-54.csv\n","                # classifier training file: Data/COMBINED_Type3-Freq10-Labeled_Motion-sessions_23-24_Fall.csv\n","            case _:\n","                raise NotImplementedError(f\"JFSKLoader is not equipped to open {file_extension} files.\")\n","\n","\n","        # 2. split dataset into data and labels\n","\n","        # Get data and labels from dataframe\n","        data = dataframe.iloc[:, :input_num].to_numpy()  # x, y, z data\n","        labels = dataframe.iloc[:, input_num:].to_numpy()  # labels\n","\n","\n","        # 3. group\n","        self.sequence_length = sequence_length\n","        g_dataset = JFAccelDataset(data, labels, sequence_length)\n","        if sequence_length is not None and sequence_length > 0:\n","            g_dataset.group()\n","\n","\n","        # 4. randomize and prepare\n","        data_train, data_test, labels_train, labels_test = train_test_split(g_dataset.data, g_dataset.labels, test_size=0.2, random_state=42)\n","\n","        # Convert to Dataset\n","        train_dataset = JFAccelDataset(data_train, labels_train, self.sequence_length)\n","        test_dataset = JFAccelDataset(data_test, labels_test, self.sequence_length)\n","\n","\n","        # other conversions happen at time of creating dataloaders\n","        # # Convert data to tensors\n","        # data_train = torch.tensor(np.array(data_train), dtype=torch.float32)  \n","        # data_test = torch.tensor(np.array(data_test), dtype=torch.float32)\n","\n","        # # Convert labels to tensors and get max index (assuming one-hot encoding)\n","        # labels_train = torch.argmax(torch.tensor(np.array(labels_train), dtype=torch.float32), dim=1)\n","        # labels_test = torch.argmax(torch.tensor(np.array(labels_test), dtype=torch.float32), dim=1)\n","\n","\n","        # 5. make adjustments related to the data\n","        self.handler = SKDatasetHandler((train_dataset, test_dataset), classification_type, SKDatasetHandler.NDARRAY_JFAD_PAIR_DTYPE, False, \"Inner\", \\\n","                                        description = \"Type 3, Unchanged\")\n","        self.__dataloader_dict = {}\n","        # uses the more general format_index\n","        self.current_dataloaders = format_index(0, SKDatasetHandler.NDARRAY_JFAD_PAIR_DTYPE[1])\n","        self.__selected_dataset_types = None\n","        # THIS is where we would use SKInputConverter\n","\n","        # self.handler.apply(lambda ds: print(np.shape(ds.data)), save = False)\n","        # 6. make adjustments related to the labels\n","        self.handler.repress_classes(*args, **kwargs)\n","        self.handler.description = \"Type 3, StatAndOth Repressed\"\n","        rep_index = self.handler.log_state(True, True, False)\n","        # self.handler.apply(lambda ds: print(np.shape(ds.data)), save = False)\n","        assert rep_index == 1, \"It's okay if you create more datasets in JFSKLoader, but make sure self.current_dataloaders is changed accordingly\"\n","        if repress_classes:\n","        # if args and args[0]:\n","        #     if len(args) > 1:\n","        #         self.repress_classes(*(args[1:]), **kwargs)\n","        #     else:\n","        #         self.repress_classes(**kwargs)\n","        # elif kwargs.pop(\"repress_classes\", False):\n","            self.current_dataloaders = (rep_index, 0)\n","\n","\n","        # 5. create initial dataloader for full dataset\n","\n","        # These are now handled in get_dataloaders(), which automatically pulls\n","            # from self.handler if there's no corresponding entry in self.__dataloader_dict\n","        # train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","        # test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","        # self.__dataloader_dict = [(self.dtype_str, train_dataloader, test_dataloader)]\n","            \n","\n","\n","\n","    def __iter__(self):\n","        return chain.from_iterable(self.get_dataloaders(None, self.__selected_dataset_types, None, self.feedback))\n","\n","\n","\n","    def select(self, dataset_types = None):\n","        self.__selected_dataset_types = dataset_types\n","        return self\n","    \n","\n","\n","    def create_dataloader(dataset, input_is_argmax = False):\n","        new_dataset = JFAccelDataset(deepcopy(dataset.data), deepcopy(dataset.labels), dataset.sequence_length)\n","        new_dataset.data = torch.tensor(new_dataset.data, dtype=torch.float32) \n","        new_dataset.labels = torch.tensor(new_dataset.labels, dtype=torch.float32)\n","        if not input_is_argmax:\n","            new_dataset.labels = torch.argmax(new_dataset.labels, dim=1)\n","        new_dataset = DataLoader(new_dataset, batch_size=64, shuffle=True)\n","        return new_dataset\n","\n","\n","\n","    def format_index(self, index):\n","        test_index = iterable_info(index)[0][0]\n","        state = self.__dataloader_dict.get(test_index, None)\n","        if state is None:\n","            state = self.handler.get_state(test_index, False)\n","            if state is None:\n","                if index is None:\n","                    return self.current_dataloaders\n","                print(f\"WARNING: index {index} does not correspond to any valid state\")\n","                return index\n","        # this is a different, more general format_index\n","        return format_index(index, state[\"ds_format\"][1], self.current_dataloaders)\n","\n","\n","\n","    # def use_dc(self, func, *args, **kwargs):\n","    #     try:\n","    #         val = func(self.handler, *args, **kwargs)\n","    #     except Exception as e:\n","    #         print(f\"WARNING: Encountered Exception in JFSKLoader.use_dc({\", \".join([func.__name__, *args, repr(kwargs)])}), see error below: \\n{e}\")\n","    #         val = None\n","    #     return val\n","    \n","\n","    # indices = <current>, dataset_types = <all>, feedback = self.feedback\n","    # @test_return(None, \"Returning dataloaders: \")\n","    def get_dataloaders(self, indices = None, dataset_types = None, return_indices = False, feedback = None):\n","        if feedback is None:\n","            feedback = self.feedback\n","        ds_format = None\n","        ret_dataloaders = []\n","        ret_state = {}\n","        state_slices = []\n","\n","        if indices is None:\n","            indices = self.current_dataloaders\n","        # if type(indices) is not tuple#:\n","        indices = list(iterable_info(indices)[0])\n","        ind_is_int = len(indices) == 1\n","        for i, index in enumerate(indices):\n","            state = self.__dataloader_dict.get(index, None)\n","            if state is None:\n","                try:\n","                    if index is None:\n","                        index = self.handler.log_state(True, True, feedback)\n","                    state = self.handler.get_state(index, feedback)\n","                except IndexError as ie:\n","                    print(f\"WARNING: encountered invalid index {index} based on index {i} in {indices} in JFSKLoader.get_dataloader(), see error below: \\n{ie}\")\n","                    try:\n","                        if index is None:\n","                            raise ie\n","                        index = self.handler.log_state(True, True, feedback)\n","                        state = self.handler.get_state(index, feedback)\n","                    except Exception as e:\n","                        raise type(e)(f\"ERROR: Failed to access any dataloader, see error below: \\n{e}\")\n","                state = deepcopy(state)\n","                datasets = state.pop(\"datasets\")\n","                if state[\"ds_format\"] not in SKDatasetHandler.VALID_DATASET_DATATYPES:\n","                    print(f\"WARNING: '{state['ds_format']}' is invalid dataset datatype. Reloading same data.\")\n","                    loop_dataloaders = self.get_dataloaders(None, None, False, feedback)\n","                else:\n","                    loop_dataloaders = self.handler.apply(JFSKLoader.create_dataloader, datasets, False, False)\n","                    state[\"dataloaders\"] = loop_dataloaders\n","                    state[\"is_argmax_format\"] = True\n","                    self.__dataloader_dict[index] = state\n","                if ds_format is None:\n","                    # if we do give it a different value here due to the \"format is not None\" conditional,\n","                        # we know we won't be checking against it next iteration... because there won't be a next iteration :D\n","                    ds_format = state[\"ds_format\"][1] # if state[\"ds_format\"][1] is not None else 1\n","                elif ds_format != state[\"ds_format\"][1]:\n","                    raise TypeError(f\"ERROR: index {index} has a different dataset format size to index {indices[0]}'s: {state['ds_format'][1]} versus {ds_format}\\n\\\n","                                    Index {indices[0]}'s dataset description: {self.__dataloader_dict.get(indices[0],{}).get('description','')}\\n\\\n","                                    Index {index}'s dataset description: {self.__dataloader_dict.get(index,{}).get('description','')}\") #\\n\\\n","                                    # Entire dataloader dict: {self.__dataloader_dict}\")\n","            else:\n","                loop_dataloaders = state[\"dataloaders\"]\n","            # self.current_dataloader = index\n","            if ind_is_int or state[\"ds_format\"][1] in (1, None):\n","                indices = (index,)\n","                ret_dataloaders = loop_dataloaders\n","                ret_state = state\n","            else:\n","                indices[i] = index\n","                ret_dataloaders.append(loop_dataloaders[i])\n","                state_slices.append(SKDatasetHandler.get_state_slice(state, i))\n","\n","        if not ret_state:\n","            ret_state[\"ds_format\"] = state_slices[0][\"ds_format\"]\n","\n","        indices = tuple(indices)\n","        if len(indices) == 1:\n","            indices = self.format_index(indices)\n","        self.current_dataloaders = indices\n","        self.__selected_dataset_types = dataset_types\n","\n","        if ret_state[\"ds_format\"][1] is None:\n","            return ret_dataloaders\n","        ret_dataloaders = tuple(ret_dataloaders)\n","        if dataset_types is None:\n","            return ret_dataloaders\n","        if return_indices is None:\n","            return tuple([dl for dl, dst in tuple(zip(ret_dataloaders, state_slices)) \\\n","                          if (dst[\"dataset_types\"] == dataset_types if type(dataset_types) is str else dst[\"dataset_types\"] in dataset_types)])\n","        return tuple(zip([(dl, i) for i, dl, dst in enumerate(tuple(zip(ret_dataloaders, state_slices))) \\\n","                          if (dst[\"dataset_types\"] == dataset_types if type(dataset_types) is str else dst[\"dataset_types\"] in dataset_types)]))\n","            "]},{"cell_type":"code","execution_count":null,"id":"e303230b","metadata":{},"outputs":[],"source":["class SKLabelConverter:\n","\n","    VALID_TYPE_CONVERSIONS = (\n","        (3, 5),\n","    )\n","    OVERRIDE_TYPE_VALIDATION = (\n","        # This is here only for conversions that\n","            # fail validate_class_type_conversion() but not for simple reasons\n","            # (simple reasons like accidentally choosing the wrong input/output types\n","            # or not listing the correct values in the below dictionaries)\n","        # If you add an entry here you may have to change logic of other parts of the code\n","            # for instance if the number of columns of the output spreadsheet will be more\n","            # than the input spreadsheet, you may have to change the dataframe.drop line at the end\n","    )\n","\n","\n","\n","    # Currently these do nothing. If we later change how we want the buffers to function\n","        # (not the length of the buffers but which buffers overlap into other classes),\n","        # we will be able to do so using this\n","    VALID_BUFFER_TYPE_CONVERSIONS = ()\n","    OVERRIDE_BUFFER_TYPE_VALIDATION = (\n","        # This is here only for BufferType conversions that\n","            # fail validate_buffer_type_conversion() but not for simple reasons\n","            # (simple reasons like accidentally choosing the wrong input/output types\n","            # or not listing the correct values in the above dictionaries)\n","        # If you add an entry here you may have to change logic of other parts of the code\n","    )\n","\n","\n","\n","    def __init__(self, labeled_data_file = None, *args):\n","        # if not all(n == Converter.NUM_OF_LABEL_TYPES for n in (len(Converter.NUM_OF_INPUTS_PER_TYPE), len(Converter.NUM_OF_CLASSES_PER_TYPE), len(Converter.NUM_OF_OUTPUTS_PER_TYPE))):\n","        #     print(\"Converter is not usable if defining dictionaries do not match corresponding dictionaries in size.\")\n","        #     print(\"Fix and rerun the code to use the converter\")\n","        #     return\n","        if labeled_data_file is not None:\n","            self.input_file_info, self.input_specifiers = SKFileNameHandler.read_data_file_name(labeled_data_file)\n","            self.output_label_type = -1\n","            self.output_freq = -1\n","            self.output_buffer_type = -1\n","            self.output_buffer_num = -1\n","        else:\n","            self.input_file_info = None\n","            self.input_specifiers = args[0]\n","            self.output_label_type = -1\n","            self.output_freq = -1\n","            self.output_buffer_type = -1\n","            self.output_buffer_num = -1\n","\n","        self.type_validated = False\n","        self.buffer_num_validated = False\n","\n","\n","\n","    def validate_label_type_conversion(self, input_label_type, output_label_type):\n","        # validating type values' consistency\n","        # input_label_type matches self.input_label_type\n","        matches_input_file = self.input_specifiers.get(SKDescriptors.CLASSIFICATION_TYPE_FS, -1) == input_label_type\n","        # input_label_type is valid and corresponds to self.with_class_num\n","        input_with_class_num = self.input_specifiers.get(SKDescriptors.WITH_CLASS_NUMBER_FS, -1)\n","        has_valid_input_type = SKDescriptors.validate_class_type(input_label_type, input_with_class_num, False)\n","        # output_label_type is valid\n","        has_valid_output_type = SKDescriptors.validate_class_type(output_label_type, 0, False)\n","        # has valid values (compared to type dictionaries and the conversion file)\n","        has_consistent_values = matches_input_file and has_valid_input_type and has_valid_output_type\n","\n","        # validating conversion logic\n","        try:\n","            _ = SKDescriptors.get_superclass_dict(input_label_type, output_label_type)\n","            has_valid_logic = True\n","        except AssertionError:\n","            print(\"WARNING: if you start doing logic that requires work without one-hot vector outputs, you may want to consider handling this invalidation differently\")\n","            has_valid_logic = False\n","\n","        # allowing override\n","        # the others are to keep someone from accidentally making a \"bad conversion,\"\n","            # but this one is to allow more-complex conversions that are possible,\n","            # given that someone manually listed the conversion in OVERRIDE_TYPE_VALIDATION\n","        # this does not override the conversion if the file is incorrect or if the types are invalid\n","        is_overridden = (input_label_type, output_label_type) in SKLabelConverter.OVERRIDE_TYPE_VALIDATION\n","\n","        return has_consistent_values and (has_valid_logic or is_overridden)\n","\n","\n","    def validate_buffer_num_conversion(self, input_buffer_num, output_buffer_num):\n","        # validate consistency\n","        # both buffer nums are non-negative\n","        has_nonnegative_buffer_nums = input_buffer_num >= 0 and output_buffer_num >= 0\n","        # input_buffer_num matches self.input_buffer_num\n","        matches_input_file = self.input_specifiers.get(SKDescriptors.BUFFER_NUMBER_FS, -1) == input_buffer_num\n","        # BufferType is valid\n","        has_valid_buffer_type = self.input_specifiers.get(SKDescriptors.BUFFER_TYPE_FS, 0) in np.arange(1, SKDescriptors.NUM_OF_BUFFER_TYPES + 1)\n","        # combining\n","        has_consistent_values = has_nonnegative_buffer_nums and matches_input_file and has_valid_buffer_type\n","        # returning\n","        return has_consistent_values\n","\n","\n","    def set_label_type_conversion(self, input_label_type, output_label_type):\n","        #NOTE that types have not yet been implemented as tuple labels\n","        if(not self.validate_label_type_conversion(input_label_type, output_label_type)):\n","            print(f\"Current object/class definitions prohibit the conversion from Type {input_label_type} to Type {output_label_type}.\")\n","            self.type_validated = False\n","            return\n","        #self.input_label_type = input_label_type\n","        self.output_label_type = output_label_type\n","        self.type_validated = True\n","\n","\n","    def set_buffer_num_conversion(self, input_buffer_num, output_buffer_num):\n","        if(not self.validate_buffer_num_conversion(input_buffer_num, output_buffer_num)):\n","            print(f\"Current object/class definitions prohibit the conversion from BufferNum {input_buffer_num} to BufferNum {output_buffer_num}.\")\n","            self.buffer_num_validated = False\n","            return\n","        #self.input_buffer_num = input_buffer_num\n","        self.output_buffer_num = output_buffer_num\n","        self.buffer_num_validated = True\n","\n","\n","\n","# df_datatype = torch.Tensor, \n","    def convert_label_type(self, input_dataframe, is_argmax_format = False, to_file = False, labels_only = True, feedback = False):\n","        if not self.type_validated:\n","            raise AssertionError(\"Yeah, no. You need to set the label type successfully before trying any conversions\")\n","\n","        input_label_type = self.input_specifiers.get(SKDescriptors.CLASSIFICATION_TYPE_FS, -1)\n","        df_datatype = type(input_dataframe)\n","\n","\n","        base_constr = lambda dataframe_like = None, size = 0: df_datatype(dataframe_like) if dataframe_like is not None else df_datatype(np.empty((size, 0)))\n","        # for indices_info, you must use iterable_info(indices)\n","        sc_inner = lambda dataframe, indices_info: np.expand_dims(dataframe[:, indices_info[0][0]], 1) if indices_info[2] == 1 else base_constr(np.concatenate([np.expand_dims(dataframe[:, i], 1) for i in indices_info[0]], axis = 1))\n","        merge_cols = lambda cols: base_constr(np.expand_dims(np.sum(cols, axis = 1), 1))\n","        append_col = lambda dataframe, col: base_constr(np.concatenate((dataframe, col), axis = 1))\n","\n","        match df_datatype:\n","            case np.ndarray:\n","                base_constr = lambda dataframe_like = None, size = 0: np.array(dataframe_like) if dataframe_like is not None else np.empty((size, 0))\n","                # for indices_info, you must use iterable_info(indices)\n","                sc_inner = lambda dataframe, indices_info: np.expand_dims(dataframe[:, indices_info[0][0]], 1) if indices_info[2] == 1 else np.concatenate([np.expand_dims(dataframe[:, i], 1) for i in indices_info[0]], axis = 1)\n","                merge_cols = lambda cols: np.expand_dims(np.sum(cols, axis = 1), 1)\n","                append_col = lambda dataframe, col: np.concatenate((dataframe, col), axis = 1)\n","            case torch.Tensor:\n","                # copy = lambda dataframe_like: dataframe_like.clone().detach().requires_grad_(dataframe_like.requires_grad)\n","                base_constr = lambda dataframe_like = None, size = 0: torch.from_numpy(dataframe_like) if dataframe_like is not None else torch.empty((size, 0), dtype=input_dataframe.dtype, layout=input_dataframe.layout, requires_grad=input_dataframe.requires_grad)\n","                # for indices_info, you must use iterable_info(indices)\n","                sc_inner = lambda dataframe, indices_info: dataframe[:, indices_info[0][0]].unsqueeze(1) if indices_info[2] == 1 else torch.cat([dataframe[:, i].unsqueeze(1) for i in indices_info[0]], dim = 1)\n","                merge_cols = lambda cols: torch.sum(cols, dim = 1).unsqueeze(1)\n","                append_col = lambda dataframe, col: torch.cat((dataframe, col), dim = 1)\n","            case _ :\n","                # raise NotImplementedError(f\"SKLabelConverter.convert_label_type is not yet equipped to handle '{(str)(df_datatype)}'.\\\n","                #                           \\n\\t\\t\\tTry using 'torch.Tensor' or implement logic for a different type\")\n","                print(f\"WARNING: SKLabelConverter.convert_label_type is not specifically equipped to handle ndarrays/dataframes of type '{(str)(df_datatype)}'.\\\n","                      \\n\\tEverything should still work, but results may be slower.\\\n","                      \\n\\tTry using dataframes of type 'np.ndarray' or 'torch.Tensor' or implement logic for a different type.\")\n","        select_cols = lambda dataframe, indices: sc_inner(dataframe, iterable_info(indices))\n","\n","\n","        if labels_only:\n","            output_dataframe = base_constr(size = np.shape(input_dataframe)[0])\n","        else:\n","            input_num = SKDescriptors.NUM_OF_INPUTS_PER_TYPE[input_label_type]\n","            output_dataframe = select_cols(input_dataframe, slice(input_num))\n","            input_dataframe = select_cols(input_dataframe, slice(input_num, input_num + SKDescriptors.NUM_OF_OUTPUTS_PER_TYPE[input_label_type]))\n","\n","\n","        if is_argmax_format:\n","            superclasses = SKDescriptors.get_superclass_dict(input_label_type, self.output_label_type)\n","            output_dataframe = base_constr(np.vectorize(superclasses.__getitem__)(input_dataframe))\n","\n","            # a, b = np.unique(np.array())\n","            # output_dataframe = \n","\n","            # output_dataframe = np.ndarray(input_dataframe.shape)\n","\n","            # output_dataframe = input_dataframe\n","            # for i in range(len(input_dataframe)):\n","            #     print(len(input_dataframe))\n","            #     print(i)\n","            #     print(input_dataframe)\n","            #     print(input_dataframe[i])\n","            #     print(\"yippee\")\n","            #     output_dataframe[i] = superclasses[input_dataframe[i]]\n","            if feedback:\n","                print(f\"Label conversion from type {input_label_type} to type {self.output_label_type} by changing output column num\")\n","            return output_dataframe\n","        \n","\n","        subclasses = SKDescriptors.get_subclass_dict(input_label_type, self.output_label_type)\n","        # print(superclasses)\n","        print(subclasses)\n","        # this is what converts the data\n","        # if you want custom conversion logic, you might want to start reworking earlier parts of the function\n","            # and leave this here since this is highly abstract and allows for most needed conversions\n","        if feedback:\n","            print(f\"Label conversion from type {input_label_type} to type {self.output_label_type}:\")\n","        for i, l in subclasses.items():\n","            # col_inds = [t[0] for t in l]\n","            if not l:\n","                raise AssertionError(\"Not only did we not find any superclasses, but we failed to detect such with our first assertion test. You will probably need to do some serious searching for this bug.\")\n","            if feedback:\n","                print(f\"\\tNew class {i} replaces old classes {l}\")\n","            old_cols = select_cols(input_dataframe, l)\n","            if len(l) > 1:\n","                new_col = merge_cols(old_cols)\n","            else:\n","                new_col = old_cols\n","            output_dataframe = append_col(output_dataframe, new_col)\n","\n","\n","        if to_file:\n","            raise NotImplementedError(\"Saving altered data labels to a file does not work currently. Reimplementation of this will hopefully be easy, but it is not a priority at the moment of writing this\")\n","        else:\n","            return output_dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":5}
